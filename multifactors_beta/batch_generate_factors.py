#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ‰¹é‡å› å­ç”Ÿæˆè„šæœ¬ ğŸš€
æ”¯æŒä¸€é”®ç”Ÿæˆæ‰€æœ‰å·²å®ç°çš„å› å­ï¼ŒåŒ…æ‹¬è´¢åŠ¡ã€æŠ€æœ¯ã€é£é™©å› å­

åŠŸèƒ½ç‰¹æ€§ï¼š
- ğŸ”¥ ä¸€é”®æ‰¹é‡ç”Ÿæˆ60+ä¸ªå› å­ 
- âš¡ å¹¶è¡Œè®¡ç®—åŠ é€Ÿ
- ğŸ“Š è¿›åº¦ç›‘æ§å’Œç»“æœéªŒè¯
- ğŸ› ï¸ çµæ´»çš„å› å­é€‰æ‹©é…ç½®
- ğŸ’¾ è‡ªåŠ¨ç»“æœä¿å­˜å’Œå¤‡ä»½

ä½¿ç”¨æ–¹å¼ï¼š
python batch_generate_factors.py --mode all                    # ç”Ÿæˆæ‰€æœ‰å› å­
python batch_generate_factors.py --mode financial             # åªç”Ÿæˆè´¢åŠ¡å› å­
python batch_generate_factors.py --factors "ROE_ttm,BP,EP"    # æŒ‡å®šå› å­
python batch_generate_factors.py --parallel 4 --fast          # 4æ ¸å¹¶è¡Œ+å¿«é€Ÿæ¨¡å¼
"""

import sys
import os
from pathlib import Path
import pandas as pd
import numpy as np
import logging
import time
import json
from datetime import datetime
from typing import Dict, List, Optional, Tuple, Any
from concurrent.futures import ProcessPoolExecutor, as_completed
import multiprocessing as mp
import argparse

# é…ç½®è·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

# å¯¼å…¥é¡¹ç›®æ¨¡å—
from config import get_config, get_config
from factors.generator.financial.pure_financial_factors import PureFinancialFactorCalculator
from factors.generator.financial.earnings_surprise_factors import SUEFactorCalculator
from factors.generator.technical.price_factors import PriceFactorCalculator  
from factors.generator.technical.volatility_factors import VolatilityFactorCalculator
from factors.generator.risk.beta_factors import BetaFactorCalculator
from factors.generator.mixed import get_mixed_factor_manager
from factors.utils.factor_calculator import FactorCalculator
from factors.base import TimeSeriesProcessor

# é…ç½®æ—¥å¿—
def setup_logging(level=logging.INFO):
    """è®¾ç½®æ—¥å¿—é…ç½®"""
    logging.basicConfig(
        level=level,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[
            logging.StreamHandler(sys.stdout),
            logging.FileHandler(f'factor_generation_{datetime.now().strftime("%Y%m%d_%H%M%S")}.log', encoding='utf-8')
        ]
    )
    return logging.getLogger(__name__)

logger = setup_logging()


class BatchFactorGenerator:
    """æ‰¹é‡å› å­ç”Ÿæˆå™¨"""
    
    # é¢„å®šä¹‰å› å­é…ç½®
    FACTOR_GROUPS = {
        'financial': [
            # ç›ˆåˆ©èƒ½åŠ›å› å­ (13ä¸ª)
            'ROE_ttm', 'ROE_lyr', 'ROA_ttm', 'ROA_lyr', 'ROIC_ttm',
            'GrossProfitMargin_ttm', 'NetProfitMargin_ttm', 'OperatingMargin_ttm',
            'EBITDAMargin_ttm', 'InterestMargin_ttm', 'CostIncomeRatio_ttm',
            
            # å¿å€ºèƒ½åŠ›å› å­ (8ä¸ª)
            'CurrentRatio', 'QuickRatio', 'CashRatio', 'DebtToAssets',
            'DebtToEquity', 'EquityMultiplier', 'InterestCoverage_ttm', 'DebtServiceCoverage_ttm',
            
            # è¥è¿æ•ˆç‡å› å­ (9ä¸ª) 
            'AssetTurnover_ttm', 'EquityTurnover_ttm', 'InventoryTurnover_ttm',
            'AccountsReceivableTurnover_ttm', 'AccountsPayableTurnover_ttm', 'CashCycle_ttm',
            'WorkingCapitalTurnover_ttm', 'FixedAssetTurnover_ttm',
            
            # æˆé•¿èƒ½åŠ›å› å­ (10ä¸ª)
            'RevenueGrowth_yoy', 'NetIncomeGrowth_yoy', 'TotalAssetsGrowth_yoy',
            'EquityGrowth_yoy', 'ROEGrowth_yoy', 'OperatingCashFlowGrowth_yoy',
            'RevenueGrowth_3y', 'NetIncomeGrowth_3y',
            
            # ç°é‡‘æµå› å­ (7ä¸ª)
            'OperatingCashFlowRatio_ttm', 'FreeCashFlowMargin_ttm', 'CashFlowToDebt_ttm',
            'OperatingCashFlowToRevenue_ttm', 'CapexToRevenue_ttm', 'CashFlowCoverage_ttm',
            
            # èµ„äº§è´¨é‡å› å­ (8ä¸ª)
            'AssetQuality', 'TangibleAssetRatio', 'GoodwillRatio', 'AccrualsRatio_ttm',
            'WorkingCapitalRatio', 'NonCurrentAssetRatio',
            
            # ç›ˆåˆ©è´¨é‡å› å­ (6ä¸ª)
            'EarningsQuality_ttm', 'AccrualQuality_ttm', 'EarningsStability_5y',
            'EarningsPersistence', 'OperatingLeverage'
        ],
        
        'technical': [
            # ä»·æ ¼å› å­
            'Price_Momentum_1M', 'Price_Momentum_3M', 'Price_Momentum_6M', 'Price_Momentum_12M',
            'Price_Reversal_1M', 'Price_Acceleration', 
            
            # æ³¢åŠ¨ç‡å› å­
            'Volatility_1M', 'Volatility_3M', 'Volatility_6M', 'Volatility_12M',
            'VolatilitySkew', 'VolatilityRatio', 'GARCH_Vol',
            
            # æŠ€æœ¯æŒ‡æ ‡
            'RSI', 'MACD', 'Bollinger_Position', 'Williams_R'
        ],
        
        'risk': [
            # Betaå› å­
            'Market_Beta', 'Market_Beta_60D', 'Market_Beta_120D', 'Market_Beta_252D',
            'Beta_Stability', 'Downside_Beta', 'Bear_Beta', 'Bull_Beta'
        ],
        
        'mixed': [
            # éœ€è¦å¤šç§æ•°æ®çš„æ··åˆå› å­
            'BP', 'EP_ttm', 'SP_ttm', 'CFP_ttm',  # ä¼°å€¼å› å­
            'SUE',  # ç›ˆä½™æƒŠå–œå› å­
            'Size', 'LogSize',  # è§„æ¨¡å› å­
        ]
    }
    
    def __init__(self, n_jobs: int = None, fast_mode: bool = False):
        """
        åˆå§‹åŒ–æ‰¹é‡å› å­ç”Ÿæˆå™¨
        
        Parameters:
        -----------
        n_jobs : int, optional
            å¹¶è¡Œè¿›ç¨‹æ•°ï¼Œé»˜è®¤ä½¿ç”¨CPUæ ¸æ•°çš„ä¸€åŠ
        fast_mode : bool
            å¿«é€Ÿæ¨¡å¼ï¼Œè·³è¿‡éƒ¨åˆ†éªŒè¯å’Œè¯¦ç»†æ—¥å¿—
        """
        self.n_jobs = n_jobs or max(1, mp.cpu_count() // 2)
        self.fast_mode = fast_mode
        self.generated_factors = {}
        self.generation_log = []
        
        # è®¾ç½®è¾“å‡ºç›®å½•
        self.output_dir = Path(get_config('main.paths.data_root')) / 'factors'
        self.output_dir.mkdir(exist_ok=True, parents=True)
        
        # æ—¥å¿—çº§åˆ«è°ƒæ•´
        if fast_mode:
            logging.getLogger().setLevel(logging.WARNING)
            
        logger.info(f"åˆå§‹åŒ–æ‰¹é‡å› å­ç”Ÿæˆå™¨: n_jobs={self.n_jobs}, fast_mode={fast_mode}")
        logger.info(f"è¾“å‡ºç›®å½•: {self.output_dir}")
    
    def load_data(self) -> Dict[str, Any]:
        """åŠ è½½æ‰€æœ‰å¿…è¦çš„æ•°æ®"""
        logger.info("ğŸ”„ å¼€å§‹åŠ è½½æ•°æ®...")
        start_time = time.time()
        
        data = {}
        raw_data_path = Path(get_config('main.paths.data_root'))
        auxiliary_path = raw_data_path / 'auxiliary'  # ç»Ÿä¸€ä½¿ç”¨StockData/auxiliary
        
        # å¿…è¦æ•°æ®æ–‡ä»¶åˆ—è¡¨
        data_files = {
            'financial_data': auxiliary_path / 'FinancialData_unified.pkl',
            'release_dates': auxiliary_path / 'ReleaseDates.pkl',
            'trading_dates': auxiliary_path / 'TradingDates.pkl',
            'price_data': raw_data_path / 'Price.pkl',
            'market_cap': auxiliary_path / 'MarketCap.pkl',  # ç§»åˆ°auxiliaryç›®å½•
        }
        
        # å¤‡ç”¨è·¯å¾„
        alt_paths = {
            'market_cap': [
                raw_data_path / 'LogMarketCap.pkl',
                raw_data_path / 'MarketCap.pkl'  # ä¿ç•™åŸè·¯å¾„ä½œä¸ºå¤‡ç”¨
            ]
        }
        
        for key, file_path in data_files.items():
            try:
                if file_path.exists():
                    data[key] = pd.read_pickle(file_path)
                    if not self.fast_mode:
                        logger.info(f"âœ… åŠ è½½ {key}: {data[key].shape}")
                elif key in alt_paths:
                    # å°è¯•å¤‡ç”¨è·¯å¾„
                    loaded = False
                    for alt_path in alt_paths[key]:
                        if alt_path.exists():
                            data[key] = pd.read_pickle(alt_path)
                            if not self.fast_mode:
                                logger.info(f"âœ… åŠ è½½ {key} (å¤‡ç”¨è·¯å¾„): {data[key].shape}")
                            loaded = True
                            break
                    if not loaded:
                        logger.warning(f"âš ï¸  æœªæ‰¾åˆ° {key} æ–‡ä»¶")
                        data[key] = None
                else:
                    logger.warning(f"âš ï¸  æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
                    data[key] = None
            except Exception as e:
                logger.error(f"âŒ åŠ è½½ {key} å¤±è´¥: {e}")
                data[key] = None
        
        # æ•°æ®é¢„å¤„ç†
        if data.get('market_cap') is not None:
            # å¤„ç†å¸‚å€¼æ•°æ®æ ¼å¼
            market_cap = data['market_cap']
            if isinstance(market_cap, pd.DataFrame):
                market_cap = market_cap.iloc[:, 0]
            # å¦‚æœæ˜¯å¯¹æ•°å¸‚å€¼ï¼Œè½¬æ¢ä¸ºåŸå§‹å€¼
            if market_cap.median() < 100:
                market_cap = np.exp(market_cap)
            data['market_cap'] = market_cap
        
        load_time = time.time() - start_time
        logger.info(f"ğŸ“Š æ•°æ®åŠ è½½å®Œæˆï¼Œè€—æ—¶ {load_time:.1f} ç§’")
        
        return data
    
    def create_factor_calculators(self) -> Dict[str, Any]:
        """åˆ›å»ºå„ç±»å› å­è®¡ç®—å™¨"""
        calculators = {}
        
        try:
            calculators['financial'] = PureFinancialFactorCalculator()
            logger.info("âœ… è´¢åŠ¡å› å­è®¡ç®—å™¨å·²åˆ›å»º")
        except Exception as e:
            logger.error(f"âŒ è´¢åŠ¡å› å­è®¡ç®—å™¨åˆ›å»ºå¤±è´¥: {e}")
            
        try:
            calculators['sue'] = SUEFactorCalculator()
            logger.info("âœ… SUEå› å­è®¡ç®—å™¨å·²åˆ›å»º")
        except Exception as e:
            logger.error(f"âŒ SUEå› å­è®¡ç®—å™¨åˆ›å»ºå¤±è´¥: {e}")
            
        try:
            calculators['technical'] = PriceFactorCalculator()
            logger.info("âœ… æŠ€æœ¯å› å­è®¡ç®—å™¨å·²åˆ›å»º")
        except Exception as e:
            logger.error(f"âŒ æŠ€æœ¯å› å­è®¡ç®—å™¨åˆ›å»ºå¤±è´¥: {e}")
            
        try:
            calculators['risk'] = BetaFactorCalculator()
            logger.info("âœ… é£é™©å› å­è®¡ç®—å™¨å·²åˆ›å»º")
        except Exception as e:
            logger.error(f"âŒ é£é™©å› å­è®¡ç®—å™¨åˆ›å»ºå¤±è´¥: {e}")
            
        try:
            calculators['mixed'] = get_mixed_factor_manager()
            logger.info("âœ… æ··åˆå› å­ç®¡ç†å™¨å·²åˆ›å»º")
        except Exception as e:
            logger.error(f"âŒ æ··åˆå› å­ç®¡ç†å™¨åˆ›å»ºå¤±è´¥: {e}")
        
        return calculators
    
    def generate_single_factor(self, factor_name: str, data: Dict[str, Any], 
                             calculators: Dict[str, Any]) -> Tuple[str, Optional[pd.Series], str]:
        """ç”Ÿæˆå•ä¸ªå› å­"""
        try:
            start_time = time.time()
            
            # æ ¹æ®å› å­åç§°ç¡®å®šä½¿ç”¨çš„è®¡ç®—å™¨å’Œæ–¹æ³•
            factor_result = None
            
            if factor_name in self.FACTOR_GROUPS['financial']:
                if 'financial' in calculators and data.get('financial_data') is not None:
                    calculator = calculators['financial']
                    if hasattr(calculator, f'calculate_{factor_name}'):
                        method = getattr(calculator, f'calculate_{factor_name}')
                        factor_result = method(data['financial_data'])
                    else:
                        # å°è¯•é€šç”¨è®¡ç®—æ–¹æ³•
                        factor_result = calculator.calculate_factor(factor_name, data['financial_data'])
                        
            elif factor_name in self.FACTOR_GROUPS['technical']:
                if 'technical' in calculators and data.get('price_data') is not None:
                    calculator = calculators['technical'] 
                    factor_result = calculator.calculate_factor(factor_name, data['price_data'])
                    
            elif factor_name in self.FACTOR_GROUPS['risk']:
                if 'risk' in calculators and data.get('price_data') is not None:
                    calculator = calculators['risk']
                    factor_result = calculator.calculate_factor(factor_name, data['price_data'])
                    
            elif factor_name in self.FACTOR_GROUPS['mixed']:
                # å¤„ç†æ··åˆå› å­
                if factor_name == 'SUE' and 'sue' in calculators:
                    factor_result = calculators['sue'].calculate_SUE(
                        data.get('financial_data'), data.get('release_dates')
                    )
                elif factor_name in ['BP', 'EP_ttm', 'SP_ttm', 'CFP_ttm']:
                    # ä¼°å€¼å› å­éœ€è¦è´¢åŠ¡å’Œå¸‚å€¼æ•°æ®
                    if ('financial' in calculators and 
                        data.get('financial_data') is not None and 
                        data.get('market_cap') is not None):
                        calculator = calculators['financial']
                        if factor_name == 'BP':
                            factor_result = calculator.calculate_BP(
                                data['financial_data'], data['market_cap']
                            )
                        elif factor_name == 'EP_ttm':
                            factor_result = calculator.calculate_EP_ttm(
                                data['financial_data'], data['market_cap']
                            )
                        # å¯ä»¥ç»§ç»­æ·»åŠ å…¶ä»–ä¼°å€¼å› å­
                elif factor_name in ['Size', 'LogSize']:
                    # è§„æ¨¡å› å­
                    if data.get('market_cap') is not None:
                        market_cap = data['market_cap']
                        if factor_name == 'Size':
                            factor_result = market_cap
                        elif factor_name == 'LogSize':
                            factor_result = np.log(market_cap)
            
            duration = time.time() - start_time
            
            if factor_result is not None and not factor_result.empty:
                message = f"âœ… {factor_name}: {factor_result.shape} ({duration:.1f}s)"
                return factor_name, factor_result, message
            else:
                message = f"âŒ {factor_name}: ç”Ÿæˆå¤±è´¥æˆ–ç»“æœä¸ºç©º"
                return factor_name, None, message
                
        except Exception as e:
            message = f"âŒ {factor_name}: å¼‚å¸¸ - {str(e)}"
            return factor_name, None, message
    
    def batch_generate(self, factor_names: List[str], 
                      parallel: bool = True) -> Dict[str, pd.Series]:
        """æ‰¹é‡ç”Ÿæˆå› å­"""
        logger.info(f"ğŸš€ å¼€å§‹æ‰¹é‡ç”Ÿæˆ {len(factor_names)} ä¸ªå› å­...")
        
        # åŠ è½½æ•°æ®
        data = self.load_data()
        
        # åˆ›å»ºè®¡ç®—å™¨
        calculators = self.create_factor_calculators()
        
        # ç”Ÿæˆå› å­
        results = {}
        generation_stats = []
        
        if parallel and len(factor_names) > 1 and self.n_jobs > 1:
            logger.info(f"âš¡ ä½¿ç”¨å¹¶è¡Œè®¡ç®— (n_jobs={self.n_jobs})")
            
            # å¹¶è¡Œç”Ÿæˆï¼ˆæ³¨æ„ï¼šéœ€è¦ç¡®ä¿æ•°æ®å’Œè®¡ç®—å™¨å¯ä»¥è¢«pickleåºåˆ—åŒ–ï¼‰
            # ç”±äºå¤æ‚æ€§ï¼Œè¿™é‡Œå…ˆä½¿ç”¨ä¸²è¡Œç‰ˆæœ¬ï¼Œåç»­å¯ä»¥ä¼˜åŒ–
            parallel = False
            
        if not parallel or len(factor_names) == 1:
            logger.info("ğŸ”„ ä½¿ç”¨ä¸²è¡Œè®¡ç®—")
            
            total_factors = len(factor_names)
            for i, factor_name in enumerate(factor_names, 1):
                if not self.fast_mode:
                    logger.info(f"[{i}/{total_factors}] ç”Ÿæˆå› å­: {factor_name}")
                
                factor_name, factor_data, message = self.generate_single_factor(
                    factor_name, data, calculators
                )
                
                if factor_data is not None:
                    results[factor_name] = factor_data
                    
                generation_stats.append(message)
                if not self.fast_mode:
                    logger.info(f"  {message}")
        
        # ä¿å­˜ç»“æœ
        self.generated_factors.update(results)
        self.generation_log.extend(generation_stats)
        
        logger.info(f"ğŸ¯ æ‰¹é‡ç”Ÿæˆå®Œæˆ: {len(results)}/{len(factor_names)} æˆåŠŸ")
        return results
    
    def save_factors(self, factors: Dict[str, pd.Series], 
                    suffix: str = None) -> Dict[str, str]:
        """ä¿å­˜å› å­æ•°æ®"""
        if not factors:
            logger.warning("æ²¡æœ‰å› å­æ•°æ®éœ€è¦ä¿å­˜")
            return {}
        
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        saved_files = {}
        
        for factor_name, factor_data in factors.items():
            try:
                # ç¡®å®šæ–‡ä»¶å
                if suffix:
                    filename = f"{factor_name}_{suffix}.pkl"
                else:
                    filename = f"{factor_name}.pkl"
                
                file_path = self.output_dir / filename
                
                # ä¿å­˜å› å­æ•°æ®
                factor_data.to_pickle(file_path)
                saved_files[factor_name] = str(file_path)
                
                if not self.fast_mode:
                    file_size = file_path.stat().st_size / 1024 / 1024  # MB
                    logger.info(f"ğŸ’¾ ä¿å­˜ {factor_name}: {filename} ({file_size:.1f}MB)")
                    
            except Exception as e:
                logger.error(f"âŒ ä¿å­˜ {factor_name} å¤±è´¥: {e}")
        
        # ä¿å­˜ç”Ÿæˆæ‘˜è¦
        summary = {
            'generation_time': timestamp,
            'total_factors': len(factors),
            'saved_factors': list(saved_files.keys()),
            'failed_factors': [name for name in factors.keys() if name not in saved_files],
            'output_directory': str(self.output_dir),
            'generation_log': self.generation_log
        }
        
        summary_file = self.output_dir / f'generation_summary_{timestamp}.json'
        with open(summary_file, 'w', encoding='utf-8') as f:
            json.dump(summary, f, indent=2, ensure_ascii=False)
        
        logger.info(f"ğŸ“‹ ç”Ÿæˆæ‘˜è¦å·²ä¿å­˜: {summary_file}")
        logger.info(f"ğŸ’¾ æˆåŠŸä¿å­˜ {len(saved_files)}/{len(factors)} ä¸ªå› å­")
        
        return saved_files
    
    def validate_factors(self, factors: Dict[str, pd.Series]) -> Dict[str, Any]:
        """éªŒè¯ç”Ÿæˆçš„å› å­è´¨é‡"""
        logger.info("ğŸ” éªŒè¯å› å­æ•°æ®è´¨é‡...")
        
        validation_results = {}
        
        for factor_name, factor_data in factors.items():
            try:
                stats = {
                    'name': factor_name,
                    'shape': factor_data.shape,
                    'null_count': factor_data.isnull().sum(),
                    'null_ratio': factor_data.isnull().mean(),
                    'inf_count': np.isinf(factor_data.values).sum(),
                    'unique_count': factor_data.nunique(),
                    'mean': factor_data.mean() if factor_data.dtype in ['float64', 'int64'] else None,
                    'std': factor_data.std() if factor_data.dtype in ['float64', 'int64'] else None,
                    'min': factor_data.min() if factor_data.dtype in ['float64', 'int64'] else None,
                    'max': factor_data.max() if factor_data.dtype in ['float64', 'int64'] else None,
                }
                
                # æ•°æ®è´¨é‡è¯„åˆ†
                quality_score = 100
                if stats['null_ratio'] > 0.5:
                    quality_score -= 30
                elif stats['null_ratio'] > 0.2:
                    quality_score -= 10
                    
                if stats['inf_count'] > 0:
                    quality_score -= 20
                    
                if stats['unique_count'] < 10:
                    quality_score -= 15
                
                stats['quality_score'] = max(0, quality_score)
                validation_results[factor_name] = stats
                
                if not self.fast_mode:
                    logger.info(f"ğŸ“Š {factor_name}: è´¨é‡åˆ†æ•°={quality_score}, "
                              f"ç©ºå€¼ç‡={stats['null_ratio']:.1%}, "
                              f"å”¯ä¸€å€¼={stats['unique_count']}")
                    
            except Exception as e:
                logger.error(f"âŒ éªŒè¯ {factor_name} å¤±è´¥: {e}")
                validation_results[factor_name] = {'error': str(e)}
        
        return validation_results
    
    def run(self, mode: str = 'all', factor_list: List[str] = None, 
           save_results: bool = True) -> Dict[str, pd.Series]:
        """è¿è¡Œæ‰¹é‡å› å­ç”Ÿæˆ"""
        
        print("=" * 80)
        print("ğŸš€ æ‰¹é‡å› å­ç”Ÿæˆå™¨")
        print(f"ğŸ“… å¼€å§‹æ—¶é—´: {datetime.now()}")
        print(f"âš™ï¸  æ¨¡å¼: {mode}")
        print(f"ğŸ”§ å¹¶è¡Œè¿›ç¨‹: {self.n_jobs}")
        print(f"âš¡ å¿«é€Ÿæ¨¡å¼: {self.fast_mode}")
        print("=" * 80)
        
        start_time = time.time()
        
        # ç¡®å®šè¦ç”Ÿæˆçš„å› å­åˆ—è¡¨
        if factor_list:
            factors_to_generate = factor_list
            logger.info(f"ğŸ¯ æŒ‡å®šå› å­æ¨¡å¼: {len(factors_to_generate)} ä¸ªå› å­")
        elif mode == 'all':
            factors_to_generate = []
            for group_factors in self.FACTOR_GROUPS.values():
                factors_to_generate.extend(group_factors)
            logger.info(f"ğŸŒŸ å…¨é‡æ¨¡å¼: {len(factors_to_generate)} ä¸ªå› å­")
        elif mode in self.FACTOR_GROUPS:
            factors_to_generate = self.FACTOR_GROUPS[mode]
            logger.info(f"ğŸ“¦ {mode}å› å­æ¨¡å¼: {len(factors_to_generate)} ä¸ªå› å­")
        else:
            logger.error(f"âŒ æœªçŸ¥æ¨¡å¼: {mode}")
            return {}
        
        # ç”Ÿæˆå› å­
        results = self.batch_generate(factors_to_generate, parallel=(self.n_jobs > 1))
        
        # éªŒè¯å› å­è´¨é‡
        if results and not self.fast_mode:
            validation_results = self.validate_factors(results)
        
        # ä¿å­˜ç»“æœ
        if save_results and results:
            saved_files = self.save_factors(results, suffix=mode if mode != 'all' else None)
        
        total_time = time.time() - start_time
        
        print("\n" + "=" * 80)
        print("ğŸ‰ æ‰¹é‡å› å­ç”Ÿæˆå®Œæˆ")
        print(f"â±ï¸  æ€»è€—æ—¶: {total_time:.1f} ç§’")
        print(f"âœ… æˆåŠŸç”Ÿæˆ: {len(results)} ä¸ªå› å­")
        print(f"ğŸ’¾ è¾“å‡ºç›®å½•: {self.output_dir}")
        print("=" * 80)
        
        return results


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='æ‰¹é‡å› å­ç”Ÿæˆå·¥å…·')
    parser.add_argument('--mode', choices=['all', 'financial', 'technical', 'risk', 'mixed'], 
                       default='all', help='ç”Ÿæˆæ¨¡å¼')
    parser.add_argument('--factors', type=str, help='æŒ‡å®šå› å­åˆ—è¡¨ï¼Œé€—å·åˆ†éš”')
    parser.add_argument('--parallel', type=int, default=None, help='å¹¶è¡Œè¿›ç¨‹æ•°')
    parser.add_argument('--fast', action='store_true', help='å¿«é€Ÿæ¨¡å¼ï¼Œå‡å°‘æ—¥å¿—è¾“å‡º')
    parser.add_argument('--no-save', action='store_true', help='ä¸ä¿å­˜ç»“æœæ–‡ä»¶')
    parser.add_argument('--list-factors', action='store_true', help='åˆ—å‡ºæ‰€æœ‰å¯ç”¨å› å­')
    
    args = parser.parse_args()
    
    if args.list_factors:
        print("ğŸ“‹ å¯ç”¨å› å­åˆ—è¡¨:")
        print("=" * 50)
        generator = BatchFactorGenerator()
        for group, factors in generator.FACTOR_GROUPS.items():
            print(f"\nğŸ“¦ {group.upper()} ({len(factors)}ä¸ª):")
            for i, factor in enumerate(factors, 1):
                print(f"  {i:2d}. {factor}")
        return
    
    # è§£æå› å­åˆ—è¡¨
    factor_list = None
    if args.factors:
        factor_list = [f.strip() for f in args.factors.split(',')]
        print(f"ğŸ¯ æŒ‡å®šå› å­: {factor_list}")
    
    # åˆ›å»ºç”Ÿæˆå™¨
    generator = BatchFactorGenerator(n_jobs=args.parallel, fast_mode=args.fast)
    
    # è¿è¡Œç”Ÿæˆ
    results = generator.run(
        mode=args.mode,
        factor_list=factor_list, 
        save_results=not args.no_save
    )
    
    # è¾“å‡ºç»“æœæ‘˜è¦
    if results:
        print(f"\nâœ¨ æˆåŠŸç”Ÿæˆçš„å› å­:")
        for i, factor_name in enumerate(results.keys(), 1):
            print(f"  {i:2d}. {factor_name}")


if __name__ == "__main__":
    main()