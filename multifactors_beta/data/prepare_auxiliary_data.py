#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è¾…åŠ©æ•°æ®å‡†å¤‡è„šæœ¬
ç”Ÿæˆå› å­è®¡ç®—æ‰€éœ€çš„è¾…åŠ©æ•°æ®æ–‡ä»¶ï¼š
1. è´¢æŠ¥å‘å¸ƒæ—¥æœŸæ•°æ®
2. äº¤æ˜“æ—¥æœŸåˆ—è¡¨
3. è‚¡ç¥¨åŸºæœ¬ä¿¡æ¯

æ³¨æ„ï¼šåŸºäºæ­£ç¡®çš„å­—æ®µç†è§£
- reportday: è´¢æŠ¥å…¬å¸ƒæ—¥æœŸ
- tradingday: è´¢æŠ¥æˆªæ­¢æ—¥æœŸï¼ˆåç§°è¯¯å¯¼ï¼‰
- d_year + d_quarter: è´¢æŠ¥æœŸé—´æ ‡è¯†
"""
import pandas as pd
import numpy as np
from pathlib import Path
import logging
from datetime import datetime
from typing import Dict, List, Optional, Tuple

# å¯¼å…¥æ”¶ç›Šç‡è®¡ç®—å™¨
try:
    from ..processor.return_calculator import ReturnCalculator
    from ..processor.price_processor import PriceDataProcessor
except ImportError:
    # å½“ä½œä¸ºä¸»æ¨¡å—è¿è¡Œæ—¶çš„å¯¼å…¥æ–¹å¼
    import sys
    from pathlib import Path
    sys.path.insert(0, str(Path(__file__).parent.parent))
    from data.processor.return_calculator import ReturnCalculator
    from data.processor.price_processor import PriceDataProcessor

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


class AuxiliaryDataPreparer:
    """è¾…åŠ©æ•°æ®å‡†å¤‡å™¨"""
    
    def __init__(self, raw_data_path: str, output_path: str):
        """
        åˆå§‹åŒ–
        
        Parameters:
        -----------
        raw_data_path : str
            åŸå§‹æ•°æ®è·¯å¾„
        output_path : str
            è¾“å‡ºæ•°æ®è·¯å¾„
        """
        self.raw_data_path = Path(raw_data_path)
        self.output_path = Path(output_path)
        self.output_path.mkdir(parents=True, exist_ok=True)
        
        # åˆå§‹åŒ–æ”¶ç›Šç‡è®¡ç®—å™¨å’Œä»·æ ¼å¤„ç†å™¨
        self.return_calculator = ReturnCalculator()
        self.price_processor = PriceDataProcessor()
        
    def _get_report_period_date(self, year: int, quarter: int) -> pd.Timestamp:
        """
        æ ¹æ®å¹´ä»½å’Œå­£åº¦è·å–è´¢æŠ¥æˆªæ­¢æ—¥æœŸ
        
        Parameters:
        -----------
        year : int
            å¹´ä»½
        quarter : int
            å­£åº¦ (1-4)
            
        Returns:
        --------
        è´¢æŠ¥æˆªæ­¢æ—¥æœŸ
        """
        quarter_end_dates = {
            1: f"{year}-03-31",
            2: f"{year}-06-30", 
            3: f"{year}-09-30",
            4: f"{year}-12-31"
        }
        return pd.Timestamp(quarter_end_dates[int(quarter)])
        
    def prepare_release_dates(self) -> pd.DataFrame:
        """
        å‡†å¤‡è´¢æŠ¥å‘å¸ƒæ—¥æœŸæ•°æ®
        
        ä»è´¢åŠ¡æ•°æ®ä¸­æå–æŠ¥è¡¨å‘å¸ƒæ—¥æœŸä¿¡æ¯
        æ³¨æ„ï¼šreportday æ˜¯å…¬å¸ƒæ—¥æœŸï¼Œä½œä¸º ReleasedDates
        """
        logger.info("å‡†å¤‡è´¢æŠ¥å‘å¸ƒæ—¥æœŸæ•°æ®...")
        
        try:
            financial_files = ['lrb.pkl', 'xjlb.pkl', 'fzb.pkl']
            release_dates_list = []
            
            for file_name in financial_files:
                file_path = self.raw_data_path / file_name
                if not file_path.exists():
                    logger.warning(f"è´¢åŠ¡æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
                    continue
                    
                logger.info(f"å¤„ç† {file_name}...")
                df = pd.read_pickle(file_path)
                
                # æ£€æŸ¥å¿…è¦çš„åˆ—
                required_cols = ['reportday', 'code', 'd_year', 'd_quarter']
                if not all(col in df.columns for col in required_cols):
                    logger.warning(f"{file_name} ç¼ºå°‘å¿…è¦çš„åˆ—")
                    continue
                
                # åˆ›å»ºè´¢æŠ¥æœŸé—´æ ‡è¯†
                df['ReportPeriod'] = df.apply(
                    lambda row: self._get_report_period_date(row['d_year'], row['d_quarter']), 
                    axis=1
                )
                
                # æå–å‘å¸ƒæ—¥æœŸä¿¡æ¯
                release_info = df[['ReportPeriod', 'code', 'reportday']].copy()
                release_info['ReleasedDates'] = pd.to_datetime(release_info['reportday'])
                release_info = release_info[['ReportPeriod', 'code', 'ReleasedDates']]
                
                # å»é‡ - åŒä¸€è´¢æŠ¥æœŸé—´åŒä¸€è‚¡ç¥¨åªä¿ç•™ä¸€æ¡è®°å½•
                release_info = release_info.drop_duplicates(['ReportPeriod', 'code'])
                release_dates_list.append(release_info)
                
            # åˆå¹¶æ‰€æœ‰æ•°æ®
            if release_dates_list:
                release_dates = pd.concat(release_dates_list, ignore_index=True)
                release_dates = release_dates.drop_duplicates(['ReportPeriod', 'code'])
                
                # è®¾ç½®ç´¢å¼•ä¸ºè´¢æŠ¥æœŸé—´
                release_dates = release_dates.set_index(['ReportPeriod', 'code'])
                release_dates.index.names = ['ReportDates', 'StockCodes']
                
                # ä¿å­˜
                output_file = self.output_path / 'ReleaseDates.pkl'
                release_dates.to_pickle(output_file)
                logger.info(f"è´¢æŠ¥å‘å¸ƒæ—¥æœŸæ•°æ®å·²ä¿å­˜: {output_file}")
                logger.info(f"æ•°æ®å½¢çŠ¶: {release_dates.shape}")
                
                return release_dates
            else:
                logger.error("æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„è´¢åŠ¡æ•°æ®")
                return pd.DataFrame()
                
        except Exception as e:
            logger.error(f"å‡†å¤‡è´¢æŠ¥å‘å¸ƒæ—¥æœŸæ•°æ®å¤±è´¥: {e}")
            return pd.DataFrame()
            
    def prepare_trading_dates(self) -> pd.Series:
        """
        å‡†å¤‡äº¤æ˜“æ—¥æœŸåˆ—è¡¨
        
        ä»ä»·æ ¼æ•°æ®ä¸­æå–äº¤æ˜“æ—¥æœŸ
        """
        logger.info("å‡†å¤‡äº¤æ˜“æ—¥æœŸæ•°æ®...")
        
        try:
            # åŠ è½½ä»·æ ¼æ•°æ®
            price_file = self.raw_data_path / 'Price.pkl'
            if not price_file.exists():
                logger.error(f"ä»·æ ¼æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {price_file}")
                return pd.Series()
                
            price_data = pd.read_pickle(price_file)
            
            # æå–äº¤æ˜“æ—¥æœŸ
            if isinstance(price_data.index, pd.MultiIndex):
                trading_dates = price_data.index.get_level_values('TradingDates').unique().sort_values()
            else:
                # å‡è®¾ç¬¬ä¸€åˆ—æ˜¯æ—¥æœŸ
                if 'tradingday' in price_data.columns:
                    trading_dates = pd.to_datetime(price_data['tradingday']).unique()
                    trading_dates = pd.Series(trading_dates).sort_values()
                else:
                    logger.error("æ— æ³•ä»ä»·æ ¼æ•°æ®ä¸­æå–äº¤æ˜“æ—¥æœŸ")
                    return pd.Series()
                    
            # ä¿å­˜
            output_file = self.output_path / 'TradingDates.pkl'
            pd.Series(trading_dates).to_pickle(output_file)
            logger.info(f"äº¤æ˜“æ—¥æœŸæ•°æ®å·²ä¿å­˜: {output_file}")
            logger.info(f"æ—¥æœŸèŒƒå›´: {trading_dates[0]} åˆ° {trading_dates[-1]}")
            logger.info(f"äº¤æ˜“æ—¥æ•°: {len(trading_dates)}")
            
            return pd.Series(trading_dates)
            
        except Exception as e:
            logger.error(f"å‡†å¤‡äº¤æ˜“æ—¥æœŸæ•°æ®å¤±è´¥: {e}")
            return pd.Series()
            
    def prepare_stock_info(self) -> pd.DataFrame:
        """
        å‡†å¤‡è‚¡ç¥¨åŸºæœ¬ä¿¡æ¯
        
        åŒ…æ‹¬è‚¡ç¥¨ä»£ç ã€åç§°ã€ä¸Šå¸‚æ—¥æœŸã€é€€å¸‚æ—¥æœŸç­‰
        """
        logger.info("å‡†å¤‡è‚¡ç¥¨åŸºæœ¬ä¿¡æ¯...")
        
        try:
            # åŠ è½½è‚¡ç¥¨åŸºæœ¬ä¿¡æ¯
            stock_info_file = self.raw_data_path / 'StockInfo.pkl'
            if stock_info_file.exists():
                stock_info = pd.read_pickle(stock_info_file)
                
                # å¤„ç†æ—¥æœŸ
                date_cols = ['list_date', 'delist_date']
                for col in date_cols:
                    if col in stock_info.columns:
                        stock_info[col] = pd.to_datetime(stock_info[col])
                        
                # è®¾ç½®ç´¢å¼•
                if 'code' in stock_info.columns:
                    stock_info = stock_info.set_index('code')
                    
                # ä¿å­˜
                output_file = self.output_path / 'StockInfo.pkl'
                stock_info.to_pickle(output_file)
                logger.info(f"è‚¡ç¥¨åŸºæœ¬ä¿¡æ¯å·²ä¿å­˜: {output_file}")
                logger.info(f"è‚¡ç¥¨æ•°é‡: {len(stock_info)}")
                
                return stock_info
            else:
                logger.warning("è‚¡ç¥¨åŸºæœ¬ä¿¡æ¯æ–‡ä»¶ä¸å­˜åœ¨ï¼Œå°†ä»å…¶ä»–æ•°æ®ä¸­æå–")
                
                # ä»ä»·æ ¼æ•°æ®ä¸­æå–è‚¡ç¥¨åˆ—è¡¨
                price_file = self.raw_data_path / 'Price.pkl'
                if price_file.exists():
                    price_data = pd.read_pickle(price_file)
                    
                    if isinstance(price_data.index, pd.MultiIndex):
                        stocks = price_data.index.get_level_values('StockCodes').unique()
                    else:
                        stocks = price_data['code'].unique() if 'code' in price_data.columns else []
                        
                    # åˆ›å»ºåŸºæœ¬ä¿¡æ¯DataFrame
                    stock_info = pd.DataFrame({
                        'code': stocks,
                        'name': [f'Stock_{code}' for code in stocks]  # å ä½åç§°
                    }).set_index('code')
                    
                    # ä¿å­˜
                    output_file = self.output_path / 'StockInfo.pkl'
                    stock_info.to_pickle(output_file)
                    logger.info(f"è‚¡ç¥¨åŸºæœ¬ä¿¡æ¯å·²ä¿å­˜: {output_file}")
                    logger.info(f"è‚¡ç¥¨æ•°é‡: {len(stock_info)}")
                    
                    return stock_info
                    
                return pd.DataFrame()
                
        except Exception as e:
            logger.error(f"å‡†å¤‡è‚¡ç¥¨åŸºæœ¬ä¿¡æ¯å¤±è´¥: {e}")
            return pd.DataFrame()
            
    def prepare_market_cap_data(self, trading_dates: pd.Series, stock_info: pd.DataFrame) -> pd.Series:
        """
        å‡†å¤‡å¸‚å€¼æ•°æ®ï¼ˆç”¨äºæ··åˆå› å­è®¡ç®—ï¼‰
        
        ä»Price.pklæ–‡ä»¶ä¸­è®¡ç®—çœŸå®å¸‚å€¼æ•°æ®ï¼šå¸‚å€¼ = æ”¶ç›˜ä»· Ã— æ€»è‚¡æœ¬
        
        Parameters:
        -----------
        trading_dates : pd.Series
            äº¤æ˜“æ—¥æœŸåˆ—è¡¨
        stock_info : pd.DataFrame
            è‚¡ç¥¨åŸºæœ¬ä¿¡æ¯
            
        Returns:
        --------
        pd.Series
            å¸‚å€¼æ•°æ®ï¼ŒMultiIndexæ ¼å¼ (ReportDates, StockCodes)
        """
        logger.info("è®¡ç®—çœŸå®å¸‚å€¼æ•°æ®ï¼ˆæ”¶ç›˜ä»· Ã— æ€»è‚¡æœ¬ï¼‰")
        
        # å°è¯•ä»Price.pklæ–‡ä»¶è¯»å–ä»·æ ¼æ•°æ®
        price_file = self.raw_data_path / 'Price.pkl'
        
        if price_file.exists():
            try:
                logger.info(f"è¯»å–ä»·æ ¼æ•°æ®æ–‡ä»¶: {price_file}")
                price_data = pd.read_pickle(price_file)
                
                # æ£€æŸ¥æ˜¯å¦æœ‰å¿…è¦çš„åˆ—
                required_cols = ['c', 'total_shares']  # æ”¶ç›˜ä»·å’Œæ€»è‚¡æœ¬
                if all(col in price_data.columns for col in required_cols):
                    # è®¡ç®—å¸‚å€¼ = æ”¶ç›˜ä»· * æ€»è‚¡æœ¬
                    # æ³¨æ„ï¼štotal_shareså•ä½é€šå¸¸æ˜¯è‚¡ï¼Œcæ˜¯å…ƒï¼Œæ‰€ä»¥å¸‚å€¼å•ä½æ˜¯å…ƒ
                    market_cap_raw = price_data['c'] * price_data['total_shares']
                    
                    # è½¬æ¢ä¸ºä¸‡å…ƒå•ä½ï¼ˆä¸æ¨¡æ‹Ÿæ•°æ®ä¿æŒä¸€è‡´ï¼‰
                    market_cap_raw = market_cap_raw / 10000
                    
                    # é‡å‘½åSerieså¹¶ç¡®ä¿æ­£ç¡®çš„ç´¢å¼•åç§°
                    market_cap = market_cap_raw.copy()
                    market_cap.name = 'market_cap'
                    
                    # ç¡®ä¿ç´¢å¼•åç§°ä¸è´¢åŠ¡æ•°æ®ä¸€è‡´
                    if market_cap.index.names != ['ReportDates', 'StockCodes']:
                        market_cap.index.names = ['ReportDates', 'StockCodes']
                    
                    # è¿‡æ»¤æ‰æ— æ•ˆå€¼
                    market_cap = market_cap.dropna()
                    market_cap = market_cap[market_cap > 0]  # ç§»é™¤éæ­£å¸‚å€¼
                    
                    logger.info(f"âœ… æˆåŠŸè®¡ç®—çœŸå®å¸‚å€¼æ•°æ®")
                    logger.info(f"   - æœ‰æ•ˆæ•°æ®é‡: {len(market_cap):,}")
                    logger.info(f"   - å¸‚å€¼èŒƒå›´: {market_cap.min():.0f} è‡³ {market_cap.max():.0f} ä¸‡å…ƒ")
                    logger.info(f"   - å¹³å‡å¸‚å€¼: {market_cap.mean():.0f} ä¸‡å…ƒ")
                    
                else:
                    logger.warning(f"ä»·æ ¼æ•°æ®ç¼ºå°‘å¿…è¦åˆ—: {required_cols}ï¼Œå›é€€åˆ°æ¨¡æ‹Ÿæ•°æ®")
                    raise ValueError("ç¼ºå°‘å¸‚å€¼è®¡ç®—å¿…è¦åˆ—")
                    
            except Exception as e:
                logger.error(f"è¯»å–ä»·æ ¼æ•°æ®å¤±è´¥: {e}ï¼Œä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®")
                return self._generate_simulated_market_cap(trading_dates, stock_info)
        
        else:
            logger.warning(f"ä»·æ ¼æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {price_file}ï¼Œä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®")
            return self._generate_simulated_market_cap(trading_dates, stock_info)
        
        # ä¿å­˜åˆ°æ–‡ä»¶
        output_file = self.output_path / 'MarketCap.pkl'
        market_cap.to_pickle(output_file)
        logger.info(f"   - ä¿å­˜è·¯å¾„: {output_file}")
        
        return market_cap
    
    def _generate_simulated_market_cap(self, trading_dates: pd.Series, stock_info: pd.DataFrame) -> pd.Series:
        """
        ç”Ÿæˆæ¨¡æ‹Ÿå¸‚å€¼æ•°æ®ï¼ˆå½“æ— æ³•è·å–çœŸå®æ•°æ®æ—¶ä½¿ç”¨ï¼‰
        
        Parameters:
        -----------
        trading_dates : pd.Series
            äº¤æ˜“æ—¥æœŸåˆ—è¡¨
        stock_info : pd.DataFrame
            è‚¡ç¥¨åŸºæœ¬ä¿¡æ¯
            
        Returns:
        --------
        pd.Series
            æ¨¡æ‹Ÿå¸‚å€¼æ•°æ®
        """
        logger.warning("âš ï¸  ç”Ÿæˆæ¨¡æ‹Ÿå¸‚å€¼æ•°æ®ï¼Œç”Ÿäº§ç¯å¢ƒåº”ä½¿ç”¨çœŸå®æ•°æ®")
        
        stock_codes = stock_info.index
        # ä½¿ç”¨æœ€è¿‘3å¹´çš„äº¤æ˜“æ—¥æœŸ
        recent_dates = trading_dates[-756:]  # çº¦3å¹´
        
        # åˆ›å»ºMultiIndex
        multi_index = pd.MultiIndex.from_product(
            [recent_dates, stock_codes], 
            names=['ReportDates', 'StockCodes']
        )
        
        # ç”Ÿæˆéšæœºå¸‚å€¼æ•°æ®
        np.random.seed(42)  # å›ºå®šéšæœºç§å­ï¼Œç¡®ä¿ç»“æœå¯å¤ç°
        market_cap_values = np.random.lognormal(
            mean=22.0,    # å¯¹æ•°å‡å€¼ï¼Œçº¦100äº¿å¸‚å€¼
            sigma=1.8,    # å¯¹æ•°æ ‡å‡†å·®ï¼Œäº§ç”Ÿåˆç†çš„åˆ†æ•£åº¦
            size=len(multi_index)
        )
        
        market_cap = pd.Series(
            market_cap_values,
            index=multi_index,
            name='market_cap'
        )
        
        logger.info(f"âœ… æ¨¡æ‹Ÿå¸‚å€¼æ•°æ®ç”Ÿæˆå®Œæˆ")
        logger.info(f"   - æ•°æ®é‡: {len(market_cap):,}")
        logger.info(f"   - æ—¥æœŸèŒƒå›´: {recent_dates.min().strftime('%Y-%m-%d')} è‡³ {recent_dates.max().strftime('%Y-%m-%d')}")
        logger.info(f"   - è‚¡ç¥¨æ•°é‡: {len(stock_codes)}")
        logger.info(f"   - å¸‚å€¼èŒƒå›´: {market_cap.min():.0f} è‡³ {market_cap.max():.0f} ä¸‡å…ƒ")
        
        return market_cap
            
    def prepare_financial_data_unified(self) -> pd.DataFrame:
        """
        å‡†å¤‡ç»Ÿä¸€æ ¼å¼çš„è´¢åŠ¡æ•°æ®
        
        å°†ä¸‰å¼ è´¢åŠ¡æŠ¥è¡¨åˆå¹¶ä¸ºç»Ÿä¸€çš„MultiIndexæ ¼å¼
        æ³¨æ„ï¼š
        - ä½¿ç”¨è´¢æŠ¥æœŸé—´ï¼ˆæ ¹æ®d_yearå’Œd_quarterè®¡ç®—ï¼‰ä½œä¸ºä¸»ç´¢å¼•
        - reportdayä½œä¸ºå‘å¸ƒæ—¥æœŸä¿å­˜åœ¨æ•°æ®åˆ—ä¸­
        """
        logger.info("å‡†å¤‡ç»Ÿä¸€æ ¼å¼çš„è´¢åŠ¡æ•°æ®...")
        
        try:
            financial_files = {
                'lrb.pkl': 'åˆ©æ¶¦è¡¨',
                'xjlb.pkl': 'ç°é‡‘æµé‡è¡¨', 
                'fzb.pkl': 'èµ„äº§è´Ÿå€ºè¡¨'
            }
            
            financial_data_list = []
            
            for file_name, table_name in financial_files.items():
                file_path = self.raw_data_path / file_name
                if not file_path.exists():
                    logger.warning(f"{table_name}æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
                    continue
                    
                logger.info(f"å¤„ç† {table_name}...")
                df = pd.read_pickle(file_path)
                
                # æ£€æŸ¥å¿…è¦åˆ—
                if 'd_year' not in df.columns or 'd_quarter' not in df.columns:
                    logger.error(f"{table_name} ç¼ºå°‘ d_year æˆ– d_quarter åˆ—")
                    continue
                
                # åˆ›å»ºè´¢æŠ¥æœŸé—´
                df['ReportPeriod'] = df.apply(
                    lambda row: self._get_report_period_date(row['d_year'], row['d_quarter']), 
                    axis=1
                )
                
                # ä¿å­˜åŸå§‹çš„å…¬å¸ƒæ—¥æœŸ
                df['ReleasedDates'] = pd.to_datetime(df['reportday'])
                
                # ä¿å­˜ tradingdayï¼ˆè™½ç„¶åå­—è¯¯å¯¼ï¼Œä½†è¿™æ˜¯åŸå§‹æ•°æ®ä¸­çš„è´¢æŠ¥æˆªæ­¢æ—¥æœŸï¼‰
                if 'tradingday' in df.columns:
                    df['OriginalTradingDay'] = pd.to_datetime(df['tradingday'])
                
                # è®¾ç½®ç´¢å¼•ä¸ºè´¢æŠ¥æœŸé—´
                if 'code' in df.columns:
                    df = df.set_index(['ReportPeriod', 'code'])
                    df.index.names = ['ReportDates', 'StockCodes']
                    
                    financial_data_list.append(df)
                    
            # åˆå¹¶è´¢åŠ¡æ•°æ®
            if financial_data_list:
                # ä½¿ç”¨å¤–è¿æ¥åˆå¹¶ï¼Œä¿ç•™æ‰€æœ‰æ•°æ®
                financial_data = financial_data_list[0]
                for df in financial_data_list[1:]:
                    # è¯†åˆ«é‡å¤åˆ—
                    common_cols = set(financial_data.columns) & set(df.columns)
                    # åªåˆå¹¶æ–°åˆ—
                    new_cols = [col for col in df.columns if col not in common_cols]
                    if new_cols:
                        financial_data = financial_data.join(df[new_cols], how='outer')
                
                # ç¡®ä¿å…³é”®åˆ—å­˜åœ¨
                if 'ReleasedDates' not in financial_data.columns:
                    logger.error("åˆå¹¶åçš„æ•°æ®ç¼ºå°‘ ReleasedDates åˆ—")
                
                # ä¿å­˜
                output_file = self.output_path / 'FinancialData_unified.pkl'
                financial_data.to_pickle(output_file)
                logger.info(f"ç»Ÿä¸€è´¢åŠ¡æ•°æ®å·²ä¿å­˜: {output_file}")
                logger.info(f"æ•°æ®å½¢çŠ¶: {financial_data.shape}")
                logger.info(f"åˆ—æ•°: {len(financial_data.columns)}")
                
                # æ£€æŸ¥æ•°æ®æ ·æœ¬
                logger.info("\næ•°æ®æ ·æœ¬æ£€æŸ¥:")
                sample = financial_data.head(3)
                for idx in sample.index:
                    logger.info(f"  è´¢æŠ¥æœŸé—´: {idx[0]}, è‚¡ç¥¨: {idx[1]}")
                    if 'ReleasedDates' in sample.columns:
                        logger.info(f"    å‘å¸ƒæ—¥æœŸ: {sample.loc[idx, 'ReleasedDates']}")
                    if 'd_year' in sample.columns and 'd_quarter' in sample.columns:
                        logger.info(f"    å¹´ä»½: {sample.loc[idx, 'd_year']}, å­£åº¦: {sample.loc[idx, 'd_quarter']}")
                
                return financial_data
            else:
                logger.error("æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„è´¢åŠ¡æ•°æ®")
                return pd.DataFrame()
                
        except Exception as e:
            logger.error(f"å‡†å¤‡ç»Ÿä¸€è´¢åŠ¡æ•°æ®å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return pd.DataFrame()
    
    def prepare_returns_data(self, trading_dates: pd.Series) -> Dict[str, pd.Series]:
        """
        å‡†å¤‡å„ç§æ”¶ç›Šç‡æ•°æ®
        
        Parameters:
        -----------
        trading_dates : pd.Series
            äº¤æ˜“æ—¥æœŸåºåˆ—
            
        Returns:
        --------
        Dict[str, pd.Series]
            æ”¶ç›Šç‡æ•°æ®å­—å…¸
        """
        logger.info("å‡†å¤‡æ”¶ç›Šç‡æ•°æ®...")
        
        try:
            # åŠ è½½ä»·æ ¼æ•°æ®
            price_file = self.raw_data_path / 'Price.pkl'
            if not price_file.exists():
                logger.error(f"ä»·æ ¼æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {price_file}")
                return {}
                
            logger.info(f"åŠ è½½ä»·æ ¼æ•°æ®: {price_file}")
            price_data = pd.read_pickle(price_file)
            
            # ç”Ÿæˆæ—¥æœŸåºåˆ—
            daily_series = self.price_processor.get_date_series(price_data, "daily")
            weekly_series = self.price_processor.get_date_series(price_data, "weekly") 
            monthly_series = self.price_processor.get_date_series(price_data, "monthly")
            
            returns_data = {}
            
            # 1. æ—¥æ”¶ç›Šç‡ (o2o)
            logger.info("è®¡ç®—æ—¥æ”¶ç›Šç‡(o2o)...")
            log_return_daily_o2o = self.return_calculator.calculate_log_return(
                price_data, daily_series, return_type="o2o"
            )
            output_file = self.output_path / 'LogReturn_daily_o2o.pkl'
            pd.to_pickle(log_return_daily_o2o, output_file)
            returns_data['daily_o2o'] = log_return_daily_o2o
            logger.info(f"æ—¥æ”¶ç›Šç‡(o2o)å·²ä¿å­˜: {output_file}")
            
            # 2. æ—¥æ”¶ç›Šç‡ (vwap)
            logger.info("è®¡ç®—æ—¥æ”¶ç›Šç‡(vwap)...")
            log_return_daily_vwap = self.return_calculator.calculate_log_return(
                price_data, daily_series, return_type="vwap"
            )
            output_file = self.output_path / 'LogReturn_daily_vwap.pkl'
            pd.to_pickle(log_return_daily_vwap, output_file)
            returns_data['daily_vwap'] = log_return_daily_vwap
            logger.info(f"æ—¥æ”¶ç›Šç‡(vwap)å·²ä¿å­˜: {output_file}")
            
            # 3. å‘¨æ”¶ç›Šç‡ (o2o)
            logger.info("è®¡ç®—å‘¨æ”¶ç›Šç‡(o2o)...")
            log_return_weekly_o2o = self.return_calculator.calculate_log_return(
                price_data, weekly_series, return_type="o2o"
            )
            output_file = self.output_path / 'LogReturn_weekly_o2o.pkl'
            pd.to_pickle(log_return_weekly_o2o, output_file)
            returns_data['weekly_o2o'] = log_return_weekly_o2o
            logger.info(f"å‘¨æ”¶ç›Šç‡(o2o)å·²ä¿å­˜: {output_file}")
            
            # 4. æœˆæ”¶ç›Šç‡ (o2o)
            logger.info("è®¡ç®—æœˆæ”¶ç›Šç‡(o2o)...")
            log_return_monthly_o2o = self.return_calculator.calculate_log_return(
                price_data, monthly_series, return_type="o2o"
            )
            output_file = self.output_path / 'LogReturn_monthly_o2o.pkl'
            pd.to_pickle(log_return_monthly_o2o, output_file)
            returns_data['monthly_o2o'] = log_return_monthly_o2o
            logger.info(f"æœˆæ”¶ç›Šç‡(o2o)å·²ä¿å­˜: {output_file}")
            
            # 5. 5å¤©æ»šåŠ¨æ”¶ç›Šç‡
            logger.info("è®¡ç®—5å¤©æ»šåŠ¨æ”¶ç›Šç‡...")
            log_return_5days = self.return_calculator.calculate_n_days_return(
                log_return_daily_o2o, lag=5
            )
            output_file = self.output_path / 'LogReturn_5days_o2o.pkl'
            pd.to_pickle(log_return_5days, output_file)
            returns_data['5days_o2o'] = log_return_5days
            logger.info(f"5å¤©æ”¶ç›Šç‡å·²ä¿å­˜: {output_file}")
            
            # 6. 20å¤©æ»šåŠ¨æ”¶ç›Šç‡
            logger.info("è®¡ç®—20å¤©æ»šåŠ¨æ”¶ç›Šç‡...")
            log_return_20days = self.return_calculator.calculate_n_days_return(
                log_return_daily_o2o, lag=20
            )
            output_file = self.output_path / 'LogReturn_20days_o2o.pkl'
            pd.to_pickle(log_return_20days, output_file)
            returns_data['20days_o2o'] = log_return_20days
            logger.info(f"20å¤©æ”¶ç›Šç‡å·²ä¿å­˜: {output_file}")
            
            logger.info(f"âœ… æ”¶ç›Šç‡æ•°æ®å‡†å¤‡å®Œæˆï¼Œå…±ç”Ÿæˆ {len(returns_data)} ç§æ”¶ç›Šç‡")
            return returns_data
            
        except Exception as e:
            logger.error(f"å‡†å¤‡æ”¶ç›Šç‡æ•°æ®å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return {}
            
    def verify_data_consistency(self):
        """éªŒè¯æ•°æ®ä¸€è‡´æ€§"""
        logger.info("\néªŒè¯æ•°æ®ä¸€è‡´æ€§...")
        
        # åŠ è½½å‡†å¤‡å¥½çš„æ•°æ®
        financial_data_file = self.output_path / 'FinancialData_unified.pkl'
        release_dates_file = self.output_path / 'ReleaseDates.pkl'
        
        if financial_data_file.exists() and release_dates_file.exists():
            financial_data = pd.read_pickle(financial_data_file)
            release_dates = pd.read_pickle(release_dates_file)
            
            # æ£€æŸ¥ç´¢å¼•æ˜¯å¦ä¸€è‡´
            logger.info("æ£€æŸ¥ç´¢å¼•ä¸€è‡´æ€§...")
            
            # è·å–ä¸€ä¸ªæ ·æœ¬è‚¡ç¥¨
            sample_stock = financial_data.index.get_level_values('StockCodes')[0]
            stock_financial = financial_data.xs(sample_stock, level='StockCodes')
            
            logger.info(f"\næ ·æœ¬è‚¡ç¥¨ {sample_stock} çš„è´¢æŠ¥æœŸé—´:")
            for report_date in stock_financial.index[:5]:
                row = stock_financial.loc[report_date]
                logger.info(f"  {report_date}:")
                if 'd_year' in row:
                    logger.info(f"    å¹´ä»½: {row['d_year']}, å­£åº¦: {row['d_quarter']}")
                if 'ReleasedDates' in row:
                    logger.info(f"    å‘å¸ƒæ—¥æœŸ: {row['ReleasedDates']}")
                    
            # æ£€æŸ¥åŒä¸€å¤©å‘å¸ƒå¤šä»½è´¢æŠ¥çš„æƒ…å†µ
            logger.info("\næ£€æŸ¥åŒå¤©å‘å¸ƒå¤šä»½è´¢æŠ¥çš„æƒ…å†µ...")
            if 'ReleasedDates' in financial_data.columns:
                # æŒ‰è‚¡ç¥¨å’Œå‘å¸ƒæ—¥æœŸåˆ†ç»„
                grouped = financial_data.reset_index().groupby(['StockCodes', 'ReleasedDates'])
                multi_reports = grouped.size()[grouped.size() > 1]
                
                if len(multi_reports) > 0:
                    logger.info(f"å‘ç° {len(multi_reports)} ä¸ªåŒå¤©å‘å¸ƒå¤šä»½è´¢æŠ¥çš„æƒ…å†µ")
                    # æ˜¾ç¤ºå‰5ä¸ªä¾‹å­
                    for (stock, release_date), count in multi_reports.head().items():
                        logger.info(f"  è‚¡ç¥¨ {stock} åœ¨ {release_date} å‘å¸ƒäº† {count} ä»½è´¢æŠ¥")
                        # æ˜¾ç¤ºå…·ä½“æ˜¯å“ªäº›è´¢æŠ¥
                        mask = (financial_data.index.get_level_values('StockCodes') == stock) & \
                               (financial_data['ReleasedDates'] == release_date)
                        reports = financial_data[mask].index.get_level_values('ReportDates')
                        for report in reports:
                            logger.info(f"    - {report}")
                else:
                    logger.info("æœªå‘ç°åŒå¤©å‘å¸ƒå¤šä»½è´¢æŠ¥çš„æƒ…å†µ")
                    
    def prepare_all(self, fast_mode: bool = False):
        """å‡†å¤‡æ‰€æœ‰è¾…åŠ©æ•°æ®"""
        logger.info("å¼€å§‹å‡†å¤‡æ‰€æœ‰è¾…åŠ©æ•°æ®...")
        logger.info(f"åŸå§‹æ•°æ®è·¯å¾„: {self.raw_data_path}")
        logger.info(f"è¾“å‡ºè·¯å¾„: {self.output_path}")
        
        if fast_mode:
            logger.info("ğŸš€ å¿«é€Ÿæ¨¡å¼å¯ç”¨ï¼šè·³è¿‡è¯¦ç»†éªŒè¯")
        
        # 1. å‡†å¤‡è´¢æŠ¥å‘å¸ƒæ—¥æœŸ
        release_dates = self.prepare_release_dates()
        
        # 2. å‡†å¤‡äº¤æ˜“æ—¥æœŸ
        trading_dates = self.prepare_trading_dates()
        
        # 3. å‡†å¤‡è‚¡ç¥¨åŸºæœ¬ä¿¡æ¯
        stock_info = self.prepare_stock_info()
        
        # 4. å‡†å¤‡ç»Ÿä¸€æ ¼å¼çš„è´¢åŠ¡æ•°æ®
        financial_data = self.prepare_financial_data_unified()
        
        # 5. å‡†å¤‡å¸‚å€¼æ•°æ®ï¼ˆç”¨äºæ··åˆå› å­è®¡ç®—ï¼‰
        market_cap = self.prepare_market_cap_data(trading_dates, stock_info)
        
        # 6. å‡†å¤‡æ”¶ç›Šç‡æ•°æ®ï¼ˆæ–°å¢ï¼‰
        returns_data = self.prepare_returns_data(trading_dates)
        
        # 7. éªŒè¯æ•°æ®ä¸€è‡´æ€§ï¼ˆå¿«é€Ÿæ¨¡å¼è·³è¿‡ï¼‰
        if not fast_mode:
            self.verify_data_consistency()
        else:
            logger.info("å¿«é€Ÿæ¨¡å¼ï¼šè·³è¿‡æ•°æ®ä¸€è‡´æ€§éªŒè¯")
        
        # ç”Ÿæˆæ•°æ®æ‘˜è¦
        summary = {
            'prepared_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            'release_dates_shape': release_dates.shape if not release_dates.empty else (0, 0),
            'trading_dates_count': len(trading_dates) if not trading_dates.empty else 0,
            'stock_info_count': len(stock_info) if not stock_info.empty else 0,
            'financial_data_shape': financial_data.shape if not financial_data.empty else (0, 0),
            'market_cap_count': len(market_cap) if not market_cap.empty else 0,
            'returns_count': len(returns_data) if returns_data else 0,
            'note': 'ä½¿ç”¨è´¢æŠ¥æœŸé—´ä½œä¸ºç´¢å¼•ï¼Œreportdayä½œä¸ºå‘å¸ƒæ—¥æœŸï¼Œæ–°å¢æ¨¡æ‹Ÿå¸‚å€¼æ•°æ®å’Œæ”¶ç›Šç‡æ•°æ®'
        }
        
        # ä¿å­˜æ‘˜è¦
        import json
        summary_file = self.output_path / 'data_preparation_summary.json'
        with open(summary_file, 'w', encoding='utf-8') as f:
            json.dump(summary, f, indent=2, ensure_ascii=False)
            
        logger.info("\nè¾…åŠ©æ•°æ®å‡†å¤‡å®Œæˆ!")
        logger.info(f"æ•°æ®æ‘˜è¦å·²ä¿å­˜: {summary_file}")
        
        return {
            'release_dates': release_dates,
            'trading_dates': trading_dates,
            'stock_info': stock_info,
            'financial_data': financial_data,
            'market_cap': market_cap,
            'summary': summary
        }


def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='è¾…åŠ©æ•°æ®å‡†å¤‡è„šæœ¬')
    parser.add_argument('--fast', action='store_true', help='å¿«é€Ÿæ¨¡å¼ï¼šè·³è¿‡æ•°æ®éªŒè¯å’Œè¯¦ç»†æ—¥å¿—')
    parser.add_argument('--test', action='store_true', help='æµ‹è¯•æ¨¡å¼ï¼šåªå¤„ç†éƒ¨åˆ†æ•°æ®')
    parser.add_argument('--parallel', action='store_true', help='å¯ç”¨å¹¶è¡Œå¤„ç†ï¼ˆå®éªŒæ€§ï¼‰')
    
    args = parser.parse_args()
    
    # æ ¹æ®å‚æ•°è°ƒæ•´æ—¥å¿—çº§åˆ«
    if args.fast:
        logging.getLogger().setLevel(logging.WARNING)
        logger.info("å¿«é€Ÿæ¨¡å¼ï¼šå·²å¯ç”¨")
    
    # é…ç½®è·¯å¾„
    raw_data_path = r"E:\Documents\PythonProject\StockProject\StockData"
    output_path = r"E:\Documents\PythonProject\StockProject\StockData\auxiliary"
    
    # åˆ›å»ºå‡†å¤‡å™¨
    preparer = AuxiliaryDataPreparer(raw_data_path, output_path)
    
    # æ ¹æ®æ¨¡å¼è¿è¡Œ
    if args.test:
        logger.info("æµ‹è¯•æ¨¡å¼ï¼šåªå¤„ç†éƒ¨åˆ†æ•°æ®")
        # å¯ä»¥åœ¨è¿™é‡Œæ·»åŠ æµ‹è¯•é€»è¾‘
    
    # å‡†å¤‡æ‰€æœ‰æ•°æ®
    results = preparer.prepare_all(fast_mode=args.fast)
    
    # æ‰“å°ç»“æœ
    print("\n" + "="*60)
    print("æ•°æ®å‡†å¤‡ç»“æœæ±‡æ€»")
    print("="*60)
    
    if 'summary' in results:
        summary = results['summary']
        print(f"å‡†å¤‡æ—¶é—´: {summary['prepared_date']}")
        print(f"è´¢æŠ¥å‘å¸ƒæ—¥æœŸæ•°æ®: {summary['release_dates_shape']}")
        print(f"äº¤æ˜“æ—¥æœŸæ•°é‡: {summary['trading_dates_count']}")
        print(f"è‚¡ç¥¨æ•°é‡: {summary['stock_info_count']}")
        print(f"ç»Ÿä¸€è´¢åŠ¡æ•°æ®: {summary['financial_data_shape']}")
        print(f"è¯´æ˜: {summary['note']}")
    
    print("\n[OK] æ‰€æœ‰è¾…åŠ©æ•°æ®å‡†å¤‡å®Œæˆ!")
    print(f"æ•°æ®ä¿å­˜è·¯å¾„: {output_path}")


if __name__ == "__main__":
    main()