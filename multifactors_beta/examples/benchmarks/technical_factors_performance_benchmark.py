#!/usr/bin/env python3
"""
æŠ€æœ¯å› å­æ€§èƒ½åŸºå‡†æµ‹è¯•

éªŒè¯å‘é‡åŒ–è®¡ç®—çš„æ•ˆæœï¼Œå¯¹æ¯”ä¼˜åŒ–å‰åçš„æ€§èƒ½å·®å¼‚
æµ‹è¯•ä¸åŒæ•°æ®è§„æ¨¡ä¸‹çš„æ€§èƒ½è¡¨ç°
"""

import sys
import os
import pandas as pd
import numpy as np
from typing import Dict, Any, List, Tuple
import time
import psutil
from pathlib import Path
import logging
import gc
import warnings
from functools import wraps

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)
warnings.filterwarnings('ignore')

# å¯¼å…¥æŠ€æœ¯å› å­æ¨¡å—
from factors.generator.technical.price_factors import MomentumFactor, MultiPeriodMomentumFactory
from factors.generator.technical.volatility_factors import HistoricalVolatilityFactor, MultiVolatilityFactory
from factors.generator.technical.oscillator_factors import RSIFactor, MultiOscillatorFactory


def memory_usage_monitor(func):
    """å†…å­˜ä½¿ç”¨ç›‘æ§è£…é¥°å™¨"""
    @wraps(func)
    def wrapper(*args, **kwargs):
        # è®°å½•å¼€å§‹æ—¶çš„å†…å­˜ä½¿ç”¨
        process = psutil.Process(os.getpid())
        start_memory = process.memory_info().rss / 1024 / 1024  # MB
        
        # æ‰§è¡Œå‡½æ•°
        result = func(*args, **kwargs)
        
        # è®°å½•ç»“æŸæ—¶çš„å†…å­˜ä½¿ç”¨
        end_memory = process.memory_info().rss / 1024 / 1024  # MB
        memory_diff = end_memory - start_memory
        
        logger.info(f"   ğŸ’¾ å†…å­˜ä½¿ç”¨: {memory_diff:.2f} MB ({start_memory:.1f} -> {end_memory:.1f} MB)")
        
        return result, memory_diff
    return wrapper


class PerformanceBenchmarkSuite:
    """æŠ€æœ¯å› å­æ€§èƒ½åŸºå‡†æµ‹è¯•å¥—ä»¶"""
    
    def __init__(self):
        self.benchmark_results = {}
        
        # æµ‹è¯•æ•°æ®è§„æ¨¡é…ç½®
        self.data_scales = {
            'small': {'days': 252, 'stocks': 50},      # 1å¹´ï¼Œ50åªè‚¡ç¥¨
            'medium': {'days': 756, 'stocks': 100},    # 3å¹´ï¼Œ100åªè‚¡ç¥¨  
            'large': {'days': 1260, 'stocks': 300},    # 5å¹´ï¼Œ300åªè‚¡ç¥¨
            'xlarge': {'days': 1512, 'stocks': 500}    # 6å¹´ï¼Œ500åªè‚¡ç¥¨
        }
    
    def create_benchmark_data(self, scale: str) -> pd.DataFrame:
        """åˆ›å»ºåŸºå‡†æµ‹è¯•æ•°æ®"""
        config = self.data_scales[scale]
        logger.info(f"åˆ›å»º{scale}è§„æ¨¡åŸºå‡†æ•°æ®: {config['days']}å¤© x {config['stocks']}åªè‚¡ç¥¨...")
        
        np.random.seed(42)
        
        # ç”Ÿæˆæ—¶é—´åºåˆ—
        end_date = pd.Timestamp('2023-12-31')
        start_date = end_date - pd.Timedelta(days=config['days'])
        dates = pd.date_range(start_date, end_date, freq='D')
        
        # ç”Ÿæˆè‚¡ç¥¨ä»£ç 
        stocks = [f'{i:06d}' for i in range(1, config['stocks'] + 1)]
        
        # åˆ›å»ºMultiIndex
        index = pd.MultiIndex.from_product([dates, stocks], names=['TradingDates', 'StockCodes'])
        n_obs = len(index)
        
        # é«˜æ•ˆçš„ä»·æ ¼æ•°æ®ç”Ÿæˆ
        logger.info(f"   ç”Ÿæˆ{n_obs:,}æ¡å¸‚åœºæ•°æ®...")
        
        # ä½¿ç”¨å‘é‡åŒ–æ“ä½œç”Ÿæˆæ‰€æœ‰ä»·æ ¼æ•°æ®
        base_prices = np.random.lognormal(mean=4, sigma=0.3, size=n_obs)
        
        # ç”Ÿæˆç›¸å…³çš„æ—¥æ”¶ç›Šç‡ï¼ˆå¸¦è‚¡ç¥¨é—´ç›¸å…³æ€§ï¼‰
        n_days = len(dates)
        n_stocks = len(stocks)
        
        # å¸‚åœºå› å­ï¼ˆå½±å“æ‰€æœ‰è‚¡ç¥¨ï¼‰
        market_returns = np.random.normal(0, 0.015, n_days)
        
        # ä¸ªè‚¡ç‰¹å¼‚æ”¶ç›Šç‡
        idiosyncratic_returns = np.random.normal(0, 0.02, (n_days, n_stocks))
        
        # ç»„åˆæˆæ€»æ”¶ç›Šç‡ï¼ˆå¸‚åœºå› å­ + ä¸ªè‚¡ç‰¹å¼‚ï¼‰
        market_impact = np.random.uniform(0.3, 0.8, n_stocks)  # æ¯åªè‚¡ç¥¨å¯¹å¸‚åœºçš„æ•æ„Ÿåº¦
        stock_returns = (market_returns[:, np.newaxis] * market_impact + idiosyncratic_returns)
        
        # å±•å¹³æˆä¸€ç»´æ•°ç»„ï¼ˆæŒ‰æ—¥æœŸ-è‚¡ç¥¨é¡ºåºï¼‰
        returns_flat = stock_returns.flatten()
        
        # è®¡ç®—ç´¯ç§¯ä»·æ ¼
        cumulative_returns = np.zeros_like(returns_flat)
        for i in range(len(stocks)):
            start_idx = i * n_days
            end_idx = (i + 1) * n_days
            cumulative_returns[start_idx:end_idx] = np.cumsum(returns_flat[start_idx:end_idx])
        
        # è®¡ç®—æœ€ç»ˆä»·æ ¼
        prices = base_prices * np.exp(cumulative_returns)
        
        # ç”ŸæˆOHLCæ•°æ®ï¼ˆå‘é‡åŒ–ï¼‰
        daily_vol = np.abs(returns_flat) + np.random.exponential(0.01, n_obs)
        
        open_prices = prices * np.exp(np.random.normal(0, daily_vol * 0.3))
        high_prices = prices * np.exp(np.abs(np.random.normal(0, daily_vol * 0.7)))
        low_prices = prices * np.exp(-np.abs(np.random.normal(0, daily_vol * 0.7)))
        
        # ç¡®ä¿ä»·æ ¼å…³ç³»æ­£ç¡®
        high_prices = np.maximum.reduce([open_prices, prices, high_prices])
        low_prices = np.minimum.reduce([open_prices, prices, low_prices])
        
        # ç”Ÿæˆæˆäº¤é‡ï¼ˆå¸¦ä»·æ ¼-æˆäº¤é‡å…³ç³»ï¼‰
        volume_base = np.random.lognormal(mean=13, sigma=1, size=n_obs)
        price_impact = np.abs(returns_flat) * 2 + 1  # ä»·æ ¼å˜åŠ¨å¤§æ—¶æˆäº¤é‡å¢åŠ 
        volumes = volume_base * price_impact
        
        # åˆ›å»ºDataFrame
        data = pd.DataFrame({
            'open': open_prices,
            'high': high_prices,
            'low': low_prices,
            'close': prices,
            'volume': volumes,
            'adjfactor': np.ones(n_obs)
        }, index=index)
        
        logger.info(f"   âœ… {scale}è§„æ¨¡æ•°æ®åˆ›å»ºå®Œæˆ: {data.shape[0]:,}æ¡è§‚æµ‹")
        return data
    
    @memory_usage_monitor
    def benchmark_single_factor_calculation(self, data: pd.DataFrame, factor_name: str, factor_creator) -> Dict[str, Any]:
        """å•å› å­è®¡ç®—æ€§èƒ½åŸºå‡†"""
        logger.info(f"   ğŸ“Š æµ‹è¯•{factor_name}...")
        
        # é¢„çƒ­ï¼ˆé¿å…é¦–æ¬¡è°ƒç”¨å¼€é”€ï¼‰
        small_data = data.iloc[:min(1000, len(data))]
        factor = factor_creator()
        _ = factor.calculate(small_data)
        
        # æ­£å¼åŸºå‡†æµ‹è¯•
        factor = factor_creator()
        start_time = time.perf_counter()
        result = factor.calculate(data)
        end_time = time.perf_counter()
        
        calc_time = end_time - start_time
        
        # è®¡ç®—æ€§èƒ½æŒ‡æ ‡
        total_obs = len(data)
        valid_obs = result.count()
        obs_per_second = total_obs / calc_time if calc_time > 0 else 0
        
        return {
            'factor_name': factor_name,
            'calc_time': calc_time,
            'total_obs': total_obs,
            'valid_obs': valid_obs,
            'obs_per_second': obs_per_second,
            'efficiency': valid_obs / calc_time if calc_time > 0 else 0,
            'result_stats': {
                'mean': result.mean(),
                'std': result.std(),
                'skew': result.skew(),
                'kurt': result.kurtosis()
            }
        }
    
    @memory_usage_monitor  
    def benchmark_batch_calculation(self, data: pd.DataFrame, batch_name: str, batch_creator) -> Dict[str, Any]:
        """æ‰¹é‡è®¡ç®—æ€§èƒ½åŸºå‡†"""
        logger.info(f"   ğŸ“Š æµ‹è¯•{batch_name}æ‰¹é‡è®¡ç®—...")
        
        # åˆ›å»ºæ‰¹é‡è®¡ç®—å™¨
        batch_calculator = batch_creator()
        
        # æ­£å¼åŸºå‡†æµ‹è¯•
        start_time = time.perf_counter()
        if 'momentum' in batch_name.lower():
            results = batch_calculator.generate_momentum_factors(data, factor_type='standard')
        elif 'volatility' in batch_name.lower():
            results = batch_calculator.generate_volatility_factors(data, factor_types=['historical', 'realized'])
        elif 'oscillator' in batch_name.lower():
            results = batch_calculator.generate_oscillator_factors(data, factor_types=['RSI', 'MACD'])
        else:
            results = {}
        end_time = time.perf_counter()
        
        calc_time = end_time - start_time
        
        # è®¡ç®—æ‰¹é‡æ€§èƒ½æŒ‡æ ‡
        factor_count = len(results)
        total_obs = len(data) * factor_count if factor_count > 0 else 0
        avg_time_per_factor = calc_time / factor_count if factor_count > 0 else 0
        
        # ç»Ÿè®¡æœ‰æ•ˆè§‚æµ‹æ•°
        total_valid_obs = sum(result.count() for result in results.values())
        
        return {
            'batch_name': batch_name,
            'calc_time': calc_time,
            'factor_count': factor_count,
            'avg_time_per_factor': avg_time_per_factor,
            'total_obs': total_obs,
            'total_valid_obs': total_valid_obs,
            'batch_efficiency': total_valid_obs / calc_time if calc_time > 0 else 0,
            'factors': list(results.keys())
        }
    
    def compare_single_vs_batch_performance(self, data: pd.DataFrame) -> Dict[str, Any]:
        """å¯¹æ¯”å•ä¸ªè®¡ç®—ä¸æ‰¹é‡è®¡ç®—çš„æ€§èƒ½"""
        logger.info("ğŸ”„ å¯¹æ¯”å•ä¸ªvsæ‰¹é‡è®¡ç®—æ€§èƒ½...")
        
        results = {}
        
        # 1. åŠ¨é‡å› å­å¯¹æ¯”
        logger.info("   æµ‹è¯•åŠ¨é‡å› å­...")
        
        # å•ä¸ªè®¡ç®—
        single_times = []
        momentum_windows = [5, 10, 20]
        
        for window in momentum_windows:
            single_result, _ = self.benchmark_single_factor_calculation(
                data, f'Momentum_{window}d', lambda: MomentumFactor(window=window)
            )
            single_times.append(single_result['calc_time'])
        
        total_single_time = sum(single_times)
        
        # æ‰¹é‡è®¡ç®—
        batch_result, _ = self.benchmark_batch_calculation(
            data, 'Momentum_Batch', lambda: MultiPeriodMomentumFactory(momentum_windows)
        )
        
        batch_time = batch_result['calc_time']
        speedup = total_single_time / batch_time if batch_time > 0 else 0
        
        results['momentum'] = {
            'single_total_time': total_single_time,
            'batch_time': batch_time,
            'speedup': speedup,
            'efficiency_gain': (speedup - 1) * 100 if speedup > 0 else 0
        }
        
        logger.info(f"     åŠ¨é‡å› å­æ‰¹é‡è®¡ç®—åŠ é€Ÿæ¯”: {speedup:.2f}x")
        
        # 2. æ³¢åŠ¨ç‡å› å­å¯¹æ¯”ï¼ˆå¦‚æœæ•°æ®è¶³å¤Ÿï¼‰
        # æš‚æ—¶è·³è¿‡æ³¢åŠ¨ç‡å› å­æµ‹è¯•
        if False:  # len(data) > 10000:  # åªåœ¨å¤§æ•°æ®é›†ä¸Šæµ‹è¯•
            logger.info("   æµ‹è¯•æ³¢åŠ¨ç‡å› å­...")
            
            vol_windows = [10, 20]
            single_vol_times = []
            
            for window in vol_windows:
                single_result, _ = self.benchmark_single_factor_calculation(
                    data, f'HistVol_{window}d', lambda: HistoricalVolatilityFactor(window=window)
                )
                single_vol_times.append(single_result['calc_time'])
            
            total_single_vol_time = sum(single_vol_times)
            
            batch_vol_result, _ = self.benchmark_batch_calculation(
                data, 'Volatility_Batch', lambda: MultiVolatilityFactory(vol_windows)
            )
            
            batch_vol_time = batch_vol_result['calc_time']
            vol_speedup = total_single_vol_time / batch_vol_time if batch_vol_time > 0 else 0
            
            results['volatility'] = {
                'single_total_time': total_single_vol_time,
                'batch_time': batch_vol_time,
                'speedup': vol_speedup,
                'efficiency_gain': (vol_speedup - 1) * 100 if vol_speedup > 0 else 0
            }
            
            logger.info(f"     æ³¢åŠ¨ç‡å› å­æ‰¹é‡è®¡ç®—åŠ é€Ÿæ¯”: {vol_speedup:.2f}x")
        
        return results
    
    def scalability_test(self) -> Dict[str, Any]:
        """å¯æ‰©å±•æ€§æµ‹è¯• - ä¸åŒæ•°æ®è§„æ¨¡ä¸‹çš„æ€§èƒ½è¡¨ç°"""
        logger.info("ğŸ“ˆ å¯æ‰©å±•æ€§æµ‹è¯•...")
        
        results = {}
        test_factor = MomentumFactor(window=20)
        
        for scale, config in self.data_scales.items():
            logger.info(f"   æµ‹è¯•{scale}è§„æ¨¡æ•°æ®...")
            
            # åˆ›å»ºæ•°æ®
            start_time = time.time()
            data = self.create_benchmark_data(scale)
            data_creation_time = time.time() - start_time
            
            # æµ‹è¯•å› å­è®¡ç®—
            try:
                single_result, memory_usage = self.benchmark_single_factor_calculation(
                    data, f'Momentum_20d_{scale}', lambda: MomentumFactor(window=20)
                )
                
                results[scale] = {
                    'config': config,
                    'data_creation_time': data_creation_time,
                    'total_observations': len(data),
                    'calc_time': single_result['calc_time'],
                    'obs_per_second': single_result['obs_per_second'],
                    'memory_usage_mb': memory_usage,
                    'valid_obs': single_result['valid_obs']
                }
                
                logger.info(f"     âœ… {scale}: {single_result['obs_per_second']:,.0f} obs/sec, {memory_usage:.1f}MB")
                
            except Exception as e:
                logger.error(f"     âŒ {scale}è§„æ¨¡æµ‹è¯•å¤±è´¥: {e}")
                results[scale] = {'error': str(e), 'config': config}
            
            # æ¸…ç†å†…å­˜
            del data
            gc.collect()
        
        return results
    
    def vectorization_efficiency_analysis(self, data: pd.DataFrame) -> Dict[str, Any]:
        """å‘é‡åŒ–æ•ˆç‡åˆ†æ"""
        logger.info("ğŸ”¢ å‘é‡åŒ–æ•ˆç‡åˆ†æ...")
        
        results = {}
        
        # æµ‹è¯•ä¸åŒçª—å£å¤§å°å¯¹æ€§èƒ½çš„å½±å“
        window_sizes = [5, 10, 20, 60, 120]
        momentum_performance = {}
        
        logger.info("   æµ‹è¯•ä¸åŒçª—å£å¤§å°çš„æ€§èƒ½å½±å“...")
        for window in window_sizes:
            try:
                result, memory_usage = self.benchmark_single_factor_calculation(
                    data, f'Momentum_{window}d', lambda: MomentumFactor(window=window)
                )
                
                momentum_performance[window] = {
                    'calc_time': result['calc_time'],
                    'obs_per_second': result['obs_per_second'],
                    'memory_usage': memory_usage
                }
                
                logger.info(f"     çª—å£{window}: {result['obs_per_second']:,.0f} obs/sec")
                
            except Exception as e:
                logger.error(f"     çª—å£{window}æµ‹è¯•å¤±è´¥: {e}")
                momentum_performance[window] = {'error': str(e)}
        
        results['window_size_impact'] = momentum_performance
        
        # åˆ†ææ€§èƒ½è¶‹åŠ¿
        valid_results = [(w, r) for w, r in momentum_performance.items() 
                        if isinstance(r, dict) and 'obs_per_second' in r]
        
        if len(valid_results) >= 2:
            windows, performances = zip(*valid_results)
            obs_per_sec_values = [r['obs_per_second'] for r in performances]
            
            # è®¡ç®—æ€§èƒ½ä¸çª—å£å¤§å°çš„å…³ç³»
            performance_trend = np.polyfit(windows, obs_per_sec_values, 1)[0]  # æ–œç‡
            
            results['performance_analysis'] = {
                'performance_trend': performance_trend,
                'interpretation': 'performance_decreases_with_window' if performance_trend < -100 else 'performance_stable'
            }
            
            logger.info(f"   æ€§èƒ½è¶‹åŠ¿: {'éšçª—å£å¢å¤§è€Œä¸‹é™' if performance_trend < -100 else 'ç›¸å¯¹ç¨³å®š'}")
        
        return results
    
    def run_comprehensive_benchmark(self) -> Dict[str, Any]:
        """è¿è¡Œç»¼åˆæ€§èƒ½åŸºå‡†æµ‹è¯•"""
        logger.info("=" * 80)
        logger.info("âš¡ æŠ€æœ¯å› å­æ€§èƒ½åŸºå‡†æµ‹è¯•å¼€å§‹")
        logger.info("=" * 80)
        
        all_results = {}
        
        try:
            # 1. å¯æ‰©å±•æ€§æµ‹è¯•
            all_results['scalability'] = self.scalability_test()
            
            # 2. ä½¿ç”¨ä¸­ç­‰è§„æ¨¡æ•°æ®è¿›è¡Œè¯¦ç»†åˆ†æ
            logger.info("åˆ›å»ºä¸­ç­‰è§„æ¨¡æ•°æ®ç”¨äºè¯¦ç»†åˆ†æ...")
            medium_data = self.create_benchmark_data('medium')
            
            # 3. å•ä¸ªvsæ‰¹é‡æ€§èƒ½å¯¹æ¯”
            all_results['batch_vs_single'] = self.compare_single_vs_batch_performance(medium_data)
            
            # 4. å‘é‡åŒ–æ•ˆç‡åˆ†æ
            all_results['vectorization'] = self.vectorization_efficiency_analysis(medium_data)
            
            # 5. ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š
            self.generate_performance_report(all_results)
            
        except Exception as e:
            logger.error(f"åŸºå‡†æµ‹è¯•è¿‡ç¨‹å‡ºç°é”™è¯¯: {e}")
            import traceback
            traceback.print_exc()
            all_results['error'] = str(e)
        
        return all_results
    
    def generate_performance_report(self, results: Dict[str, Any]):
        """ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š"""
        logger.info("\n" + "=" * 80)
        logger.info("ğŸ“Š æŠ€æœ¯å› å­æ€§èƒ½åŸºå‡†æŠ¥å‘Š")
        logger.info("=" * 80)
        
        # 1. å¯æ‰©å±•æ€§æŠ¥å‘Š
        if 'scalability' in results:
            logger.info("ğŸ“ˆ å¯æ‰©å±•æ€§æµ‹è¯•ç»“æœ:")
            
            scalability = results['scalability']
            for scale, metrics in scalability.items():
                if 'error' not in metrics:
                    config = metrics['config']
                    obs_count = metrics['total_observations']
                    calc_time = metrics['calc_time']
                    obs_per_sec = metrics['obs_per_second']
                    memory_mb = metrics['memory_usage_mb']
                    
                    logger.info(f"   {scale:>7}: {obs_count:>8,} è§‚æµ‹ | "
                              f"{calc_time:>6.2f}s | "
                              f"{obs_per_sec:>8,.0f} obs/sec | "
                              f"{memory_mb:>6.1f}MB")
        
        # 2. æ‰¹é‡è®¡ç®—æ•ˆç‡æŠ¥å‘Š
        if 'batch_vs_single' in results:
            logger.info("\nğŸ”„ æ‰¹é‡è®¡ç®—æ•ˆç‡:")
            
            batch_results = results['batch_vs_single']
            for factor_type, metrics in batch_results.items():
                if 'speedup' in metrics:
                    speedup = metrics['speedup']
                    efficiency_gain = metrics['efficiency_gain']
                    
                    logger.info(f"   {factor_type:>10}: {speedup:>5.2f}x åŠ é€Ÿ | "
                              f"{efficiency_gain:>5.1f}% æ•ˆç‡æå‡")
        
        # 3. å‘é‡åŒ–æ•ˆç‡æŠ¥å‘Š
        if 'vectorization' in results:
            logger.info("\nğŸ”¢ å‘é‡åŒ–æ•ˆç‡åˆ†æ:")
            
            vectorization = results['vectorization']
            if 'performance_analysis' in vectorization:
                analysis = vectorization['performance_analysis']
                interpretation = analysis['interpretation']
                
                if interpretation == 'performance_stable':
                    logger.info("   âœ… å‘é‡åŒ–å®ç°è‰¯å¥½ï¼Œæ€§èƒ½éšçª—å£å¤§å°ä¿æŒç¨³å®š")
                else:
                    logger.info("   âš ï¸  æ€§èƒ½éšçª—å£å¤§å°ä¸‹é™ï¼Œå¯èƒ½éœ€è¦è¿›ä¸€æ­¥ä¼˜åŒ–")
        
        # 4. æ•´ä½“è¯„ä¼°
        logger.info("\nğŸ¯ æ•´ä½“æ€§èƒ½è¯„ä¼°:")
        
        # åŸºäºmediumè§„æ¨¡æ•°æ®çš„åŸºå‡†æ€§èƒ½
        if 'scalability' in results and 'medium' in results['scalability']:
            medium_metrics = results['scalability']['medium']
            if 'obs_per_second' in medium_metrics:
                obs_per_sec = medium_metrics['obs_per_second']
                
                if obs_per_sec > 50000:
                    logger.info("   âœ… æ€§èƒ½ä¼˜ç§€ - é«˜æ•ˆçš„å‘é‡åŒ–è®¡ç®—")
                elif obs_per_sec > 20000:
                    logger.info("   âœ… æ€§èƒ½è‰¯å¥½ - æ»¡è¶³ç”Ÿäº§éœ€æ±‚")  
                elif obs_per_sec > 10000:
                    logger.info("   âš ï¸  æ€§èƒ½åŠæ ¼ - å¯ä»¥ä½¿ç”¨ä½†å»ºè®®ä¼˜åŒ–")
                else:
                    logger.info("   âŒ æ€§èƒ½è¾ƒå·® - éœ€è¦é‡å¤§ä¼˜åŒ–")
        
        # 5. ä¼˜åŒ–å»ºè®®
        logger.info("\nğŸ’¡ ä¼˜åŒ–å»ºè®®:")
        
        # åŸºäºæ‰¹é‡è®¡ç®—æ•ˆç‡ç»™å‡ºå»ºè®®
        if 'batch_vs_single' in results:
            avg_speedup = np.mean([m.get('speedup', 1) for m in results['batch_vs_single'].values() if 'speedup' in m])
            if avg_speedup > 2:
                logger.info("   âœ… æ‰¹é‡è®¡ç®—æ•ˆç‡é«˜ï¼Œå»ºè®®åœ¨ç”Ÿäº§ä¸­ä½¿ç”¨æ‰¹é‡æ–¹æ³•")
            elif avg_speedup > 1.5:
                logger.info("   âœ… æ‰¹é‡è®¡ç®—æœ‰ä¸€å®šä¼˜åŠ¿ï¼Œæ¨èä½¿ç”¨")
            else:
                logger.info("   âš ï¸  æ‰¹é‡è®¡ç®—ä¼˜åŠ¿ä¸æ˜æ˜¾ï¼Œéœ€è¦è¿›ä¸€æ­¥ä¼˜åŒ–")
        
        logger.info("   ğŸ’¡ å»ºè®®ä½¿ç”¨MultiPeriodFactoryç­‰æ‰¹é‡ç”Ÿæˆå™¨")
        logger.info("   ğŸ’¡ å¤§æ•°æ®é›†è®¡ç®—æ—¶æ³¨æ„å†…å­˜ç®¡ç†")
        logger.info("   ğŸ’¡ ç”Ÿäº§ç¯å¢ƒå»ºè®®ä½¿ç”¨mediumä»¥ä¸Šè§„æ¨¡çš„ç¼“å­˜")
        
        logger.info("\nâœ… æ€§èƒ½åŸºå‡†æµ‹è¯•å®Œæˆ!")


def main():
    """ä¸»å‡½æ•°"""
    try:
        # åˆ›å»ºæ€§èƒ½åŸºå‡†æµ‹è¯•å¥—ä»¶
        benchmark_suite = PerformanceBenchmarkSuite()
        
        # è¿è¡Œç»¼åˆåŸºå‡†æµ‹è¯•
        results = benchmark_suite.run_comprehensive_benchmark()
        
        return True
        
    except Exception as e:
        logger.error(f"æ€§èƒ½åŸºå‡†æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)