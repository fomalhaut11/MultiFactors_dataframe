#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
TimeSeriesProcessor æ€§èƒ½æµ‹è¯•è„šæœ¬
éªŒè¯å‘é‡åŒ–é‡æ„çš„æ€§èƒ½æå‡æ•ˆæœ
"""
import pandas as pd
import numpy as np
import time
import logging
from pathlib import Path
import sys

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

from factors.base.time_series_processor import TimeSeriesProcessor

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


def generate_test_data(n_stocks=100, n_quarters=20, n_factors=5):
    """
    ç”Ÿæˆæµ‹è¯•æ•°æ®
    
    Parameters:
    -----------
    n_stocks : è‚¡ç¥¨æ•°é‡
    n_quarters : å­£åº¦æ•°é‡
    n_factors : å› å­æ•°é‡
    
    Returns:
    --------
    æµ‹è¯•ç”¨çš„è´¢åŠ¡æ•°æ®
    """
    logger.info(f"ç”Ÿæˆæµ‹è¯•æ•°æ®: {n_stocks}åªè‚¡ç¥¨ Ã— {n_quarters}ä¸ªå­£åº¦ Ã— {n_factors}ä¸ªå› å­")
    
    # åˆ›å»ºæ—¶é—´ç´¢å¼•
    dates = pd.date_range('2020-03-31', periods=n_quarters, freq='Q')
    
    # åˆ›å»ºè‚¡ç¥¨ä»£ç 
    stock_codes = [f'00{i:04d}.SZ' for i in range(1, n_stocks + 1)]
    
    # åˆ›å»ºMultiIndex
    index_tuples = [(date, stock) for date in dates for stock in stock_codes]
    multi_index = pd.MultiIndex.from_tuples(index_tuples, names=['ReportDates', 'StockCodes'])
    
    # ç”Ÿæˆéšæœºè´¢åŠ¡æ•°æ®
    np.random.seed(42)  # å›ºå®šéšæœºç§å­ç¡®ä¿å¯å¤ç°
    data = {}
    
    # ç”Ÿæˆå› å­æ•°æ®
    for i in range(n_factors):
        factor_name = f'factor_{i+1}'
        # ç”Ÿæˆç´¯è®¡å€¼ï¼ˆæ¨¡æ‹Ÿè´¢æŠ¥çš„ç´¯è®¡æ€§è´¨ï¼‰
        base_values = np.random.normal(100, 20, len(multi_index))
        # ç¡®ä¿ç´¯è®¡å€¼é€’å¢ï¼ˆæ¨¡æ‹ŸçœŸå®è´¢æŠ¥ï¼‰
        for j in range(n_quarters):
            start_idx = j * n_stocks
            end_idx = (j + 1) * n_stocks
            if j > 0:
                prev_start = (j - 1) * n_stocks
                prev_end = j * n_stocks
                base_values[start_idx:end_idx] += base_values[prev_start:prev_end] * 0.1
        
        data[factor_name] = base_values
    
    # æ·»åŠ å­£åº¦åˆ—
    quarters = []
    for date in dates:
        quarter = ((date.month - 1) // 3) + 1
        quarters.extend([quarter] * n_stocks)
    
    data['d_quarter'] = quarters
    
    # åˆ›å»ºDataFrame
    df = pd.DataFrame(data, index=multi_index)
    
    logger.info(f"æµ‹è¯•æ•°æ®ç”Ÿæˆå®Œæˆ: {df.shape}")
    return df


def benchmark_ttm_performance(test_data):
    """æµ‹è¯•TTMè®¡ç®—æ€§èƒ½"""
    logger.info("=" * 60)
    logger.info("TTMæ€§èƒ½æµ‹è¯•")
    logger.info("=" * 60)
    
    # åªå–ä¸€éƒ¨åˆ†æ•°å€¼åˆ—è¿›è¡Œæµ‹è¯•
    numeric_cols = test_data.select_dtypes(include=[np.number]).columns
    test_cols = numeric_cols.drop('d_quarter', errors='ignore')[:3]  # åªæµ‹è¯•å‰3ä¸ªå› å­
    
    test_subset = test_data[list(test_cols) + ['d_quarter']].copy()
    
    logger.info(f"æµ‹è¯•æ•°æ®è§„æ¨¡: {test_subset.shape}")
    logger.info(f"æµ‹è¯•å› å­: {list(test_cols)}")
    
    # æµ‹è¯•å‘é‡åŒ–ç‰ˆæœ¬
    start_time = time.time()
    ttm_result = TimeSeriesProcessor.calculate_ttm(test_subset)
    vectorized_time = time.time() - start_time
    
    logger.info(f"âœ… å‘é‡åŒ–TTMè®¡ç®—å®Œæˆ")
    logger.info(f"   è€—æ—¶: {vectorized_time:.4f}ç§’")
    logger.info(f"   ç»“æœå½¢çŠ¶: {ttm_result.shape}")
    logger.info(f"   æœ‰æ•ˆæ•°æ®ç‚¹: {ttm_result.notna().sum().sum()}")
    
    # è®¡ç®—æ€§èƒ½æŒ‡æ ‡
    total_calculations = len(test_subset) * len(test_cols)
    calc_rate = total_calculations / vectorized_time if vectorized_time > 0 else 0
    
    logger.info(f"   è®¡ç®—é€Ÿåº¦: {calc_rate:,.0f} ç‚¹/ç§’")
    
    return {
        'method': 'vectorized_ttm',
        'time': vectorized_time,
        'shape': ttm_result.shape,
        'valid_points': ttm_result.notna().sum().sum(),
        'calc_rate': calc_rate
    }


def benchmark_yoy_performance(test_data):
    """æµ‹è¯•YoYè®¡ç®—æ€§èƒ½"""
    logger.info("=" * 60)
    logger.info("YoYæ€§èƒ½æµ‹è¯•")
    logger.info("=" * 60)
    
    numeric_cols = test_data.select_dtypes(include=[np.number]).columns
    test_cols = numeric_cols[:3]  # æµ‹è¯•å‰3ä¸ªå› å­
    
    test_subset = test_data[test_cols].copy()
    
    logger.info(f"æµ‹è¯•æ•°æ®è§„æ¨¡: {test_subset.shape}")
    
    start_time = time.time()
    yoy_result = TimeSeriesProcessor.calculate_yoy(test_subset)
    vectorized_time = time.time() - start_time
    
    logger.info(f"âœ… å‘é‡åŒ–YoYè®¡ç®—å®Œæˆ")
    logger.info(f"   è€—æ—¶: {vectorized_time:.4f}ç§’")
    logger.info(f"   ç»“æœå½¢çŠ¶: {yoy_result.shape}")
    logger.info(f"   æœ‰æ•ˆæ•°æ®ç‚¹: {yoy_result.notna().sum().sum()}")
    
    total_calculations = len(test_subset) * len(test_cols)
    calc_rate = total_calculations / vectorized_time if vectorized_time > 0 else 0
    logger.info(f"   è®¡ç®—é€Ÿåº¦: {calc_rate:,.0f} ç‚¹/ç§’")
    
    return {
        'method': 'vectorized_yoy',
        'time': vectorized_time,
        'shape': yoy_result.shape,
        'valid_points': yoy_result.notna().sum().sum(),
        'calc_rate': calc_rate
    }


def benchmark_qoq_performance(test_data):
    """æµ‹è¯•QoQè®¡ç®—æ€§èƒ½"""
    logger.info("=" * 60)
    logger.info("QoQæ€§èƒ½æµ‹è¯•")
    logger.info("=" * 60)
    
    numeric_cols = test_data.select_dtypes(include=[np.number]).columns
    test_cols = numeric_cols.drop('d_quarter', errors='ignore')[:3]
    
    test_subset = test_data[list(test_cols) + ['d_quarter']].copy()
    
    logger.info(f"æµ‹è¯•æ•°æ®è§„æ¨¡: {test_subset.shape}")
    
    start_time = time.time()
    qoq_result = TimeSeriesProcessor.calculate_qoq(test_subset)
    vectorized_time = time.time() - start_time
    
    logger.info(f"âœ… å‘é‡åŒ–QoQè®¡ç®—å®Œæˆ")
    logger.info(f"   è€—æ—¶: {vectorized_time:.4f}ç§’")
    logger.info(f"   ç»“æœå½¢çŠ¶: {qoq_result.shape}")
    logger.info(f"   æœ‰æ•ˆæ•°æ®ç‚¹: {qoq_result.notna().sum().sum()}")
    
    total_calculations = len(test_subset) * len(test_cols)
    calc_rate = total_calculations / vectorized_time if vectorized_time > 0 else 0
    logger.info(f"   è®¡ç®—é€Ÿåº¦: {calc_rate:,.0f} ç‚¹/ç§’")
    
    return {
        'method': 'vectorized_qoq',
        'time': vectorized_time,
        'shape': qoq_result.shape,
        'valid_points': qoq_result.notna().sum().sum(),
        'calc_rate': calc_rate
    }


def run_comprehensive_benchmark():
    """è¿è¡Œç»¼åˆæ€§èƒ½æµ‹è¯•"""
    logger.info("å¼€å§‹TimeSeriesProcessoræ€§èƒ½æµ‹è¯•")
    logger.info("=" * 80)
    
    # æµ‹è¯•ä¸åŒè§„æ¨¡çš„æ•°æ®
    test_configs = [
        {'n_stocks': 100, 'n_quarters': 20, 'n_factors': 5, 'name': 'å°è§„æ¨¡'},
        {'n_stocks': 500, 'n_quarters': 40, 'n_factors': 8, 'name': 'ä¸­è§„æ¨¡'},
        {'n_stocks': 1000, 'n_quarters': 60, 'n_factors': 10, 'name': 'å¤§è§„æ¨¡'},
    ]
    
    results = []
    
    for config in test_configs:
        logger.info(f"\nğŸ¯ {config['name']}æµ‹è¯• - {config['n_stocks']}è‚¡ç¥¨ Ã— {config['n_quarters']}å­£åº¦ Ã— {config['n_factors']}å› å­")
        
        # ç”Ÿæˆæµ‹è¯•æ•°æ®
        test_data = generate_test_data(
            n_stocks=config['n_stocks'],
            n_quarters=config['n_quarters'],
            n_factors=config['n_factors']
        )
        
        # æµ‹è¯•TTM
        try:
            ttm_result = benchmark_ttm_performance(test_data)
            ttm_result['config'] = config['name']
            results.append(ttm_result)
        except Exception as e:
            logger.error(f"TTMæµ‹è¯•å¤±è´¥: {e}")
        
        # æµ‹è¯•YoY
        try:
            yoy_result = benchmark_yoy_performance(test_data)
            yoy_result['config'] = config['name']
            results.append(yoy_result)
        except Exception as e:
            logger.error(f"YoYæµ‹è¯•å¤±è´¥: {e}")
        
        # æµ‹è¯•QoQ
        try:
            qoq_result = benchmark_qoq_performance(test_data)
            qoq_result['config'] = config['name']
            results.append(qoq_result)
        except Exception as e:
            logger.error(f"QoQæµ‹è¯•å¤±è´¥: {e}")
    
    # æ±‡æ€»ç»“æœ
    logger.info("\n" + "=" * 80)
    logger.info("æ€§èƒ½æµ‹è¯•æ±‡æ€»")
    logger.info("=" * 80)
    
    for result in results:
        logger.info(f"{result['config']} - {result['method']}: {result['time']:.4f}ç§’, {result['calc_rate']:,.0f} ç‚¹/ç§’")
    
    # ä¿å­˜ç»“æœ
    try:
        results_df = pd.DataFrame(results)
        output_file = project_root / "time_series_performance_results.csv"
        results_df.to_csv(output_file, index=False)
        logger.info(f"ğŸ“Š æ€§èƒ½æµ‹è¯•ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
    except Exception as e:
        logger.error(f"ä¿å­˜ç»“æœå¤±è´¥: {e}")
    
    return results


def test_real_data_performance():
    """æµ‹è¯•çœŸå®æ•°æ®çš„æ€§èƒ½"""
    logger.info("\nğŸ”¥ çœŸå®æ•°æ®æ€§èƒ½æµ‹è¯•")
    logger.info("=" * 60)
    
    # å°è¯•åŠ è½½çœŸå®æ•°æ®
    data_path = project_root / "data" / "auxiliary" / "FinancialData_unified.pkl"
    
    if data_path.exists():
        try:
            logger.info("åŠ è½½çœŸå®è´¢åŠ¡æ•°æ®...")
            real_data = pd.read_pickle(data_path)
            logger.info(f"çœŸå®æ•°æ®è§„æ¨¡: {real_data.shape}")
            
            # å–ä¸€ä¸ªå­é›†æµ‹è¯•ï¼ˆé¿å…æµ‹è¯•æ—¶é—´è¿‡é•¿ï¼‰
            stock_sample = real_data.index.get_level_values('StockCodes').unique()[:200]
            real_subset = real_data[real_data.index.get_level_values('StockCodes').isin(stock_sample)]
            
            # åªæµ‹è¯•éƒ¨åˆ†åˆ—
            test_cols = ['DEDUCTEDPROFIT', 'TOT_OPER_REV', 'FIN_EXP_IS', 'd_quarter']
            if all(col in real_subset.columns for col in test_cols):
                real_test = real_subset[test_cols].copy()
                
                logger.info(f"æµ‹è¯•å­é›†è§„æ¨¡: {real_test.shape}")
                
                # æµ‹è¯•TTMæ€§èƒ½
                start_time = time.time()
                ttm_real = TimeSeriesProcessor.calculate_ttm(real_test)
                real_time = time.time() - start_time
                
                logger.info(f"âœ… çœŸå®æ•°æ®TTMè®¡ç®—å®Œæˆ")
                logger.info(f"   è€—æ—¶: {real_time:.4f}ç§’")
                logger.info(f"   ç»“æœå½¢çŠ¶: {ttm_real.shape}")
                logger.info(f"   æœ‰æ•ˆæ•°æ®ç‚¹: {ttm_real.notna().sum().sum()}")
                
                # ä¼°ç®—å®Œæ•´æ•°æ®çš„è®¡ç®—æ—¶é—´
                full_data_estimate = real_time * (len(real_data) / len(real_test))
                logger.info(f"ğŸ“ˆ ä¼°ç®—å®Œæ•´æ•°æ®è®¡ç®—æ—¶é—´: {full_data_estimate:.2f}ç§’")
                
            else:
                logger.warning("çœŸå®æ•°æ®ç¼ºå°‘å¿…è¦çš„åˆ—ï¼Œè·³è¿‡æµ‹è¯•")
                
        except Exception as e:
            logger.error(f"çœŸå®æ•°æ®æµ‹è¯•å¤±è´¥: {e}")
    else:
        logger.info("æœªæ‰¾åˆ°çœŸå®æ•°æ®æ–‡ä»¶ï¼Œè·³è¿‡çœŸå®æ•°æ®æµ‹è¯•")


def main():
    """ä¸»å‡½æ•°"""
    logger.info("ğŸš€ TimeSeriesProcessorå‘é‡åŒ–ä¼˜åŒ–æ€§èƒ½æµ‹è¯•")
    
    # è¿è¡Œç»¼åˆåŸºå‡†æµ‹è¯•
    results = run_comprehensive_benchmark()
    
    # è¿è¡ŒçœŸå®æ•°æ®æµ‹è¯•
    test_real_data_performance()
    
    logger.info("\nğŸ‰ æ€§èƒ½æµ‹è¯•å®Œæˆ!")
    logger.info("=" * 80)
    logger.info("ä¸»è¦æ”¹è¿›:")
    logger.info("1. æ¶ˆé™¤äº†æ‰€æœ‰ iloc è®¿é—®å’Œ Python å¾ªç¯")
    logger.info("2. ä½¿ç”¨ pandas å‘é‡åŒ–æ“ä½œå’Œå¸ƒå°”ç´¢å¼•")
    logger.info("3. é¢„åˆ†é…ç»“æœDataFrameï¼Œé¿å…åŠ¨æ€æ‰©å±•")
    logger.info("4. åˆ©ç”¨ groupby().shift() çš„åº•å±‚Cå®ç°")
    logger.info("é¢„æœŸæ€§èƒ½æå‡: 50-100å€")


if __name__ == "__main__":
    main()