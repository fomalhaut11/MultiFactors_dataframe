#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å®éªŒæ€§å› å­æ¨¡å— - ç”¨äºæµ‹è¯•å’ŒéªŒè¯æ–°å› å­æƒ³æ³•

è®¾è®¡åŸåˆ™ï¼š
==========
1. ç»§æ‰¿PureFinancialFactorCalculatorï¼Œå¤ç”¨æ‰€æœ‰åŸºç¡€åŠŸèƒ½
2. å®éªŒæ€§å› å­ä¸æ³¨å†Œåˆ°æ­£å¼ç³»ç»Ÿï¼Œä¿æŒçµæ´»æ€§
3. ä½¿ç”¨EXPERIMENTAL_å‰ç¼€æ ‡è¯†å®éªŒæ€§å› å­
4. æä¾›å¿«é€ŸéªŒè¯å’Œæµ‹è¯•å·¥å…·
5. ç»è¿‡éªŒè¯çš„å› å­å¯ä»¥è½»æ¾è¿ç§»åˆ°æ­£å¼æ¨¡å—

å·¥ä½œæµç¨‹ï¼š
==========
1. åœ¨è¿™é‡Œå®ç°æ–°å› å­æƒ³æ³• -> 2. å¿«é€Ÿæµ‹è¯•éªŒè¯ -> 3. é€šè¿‡éªŒè¯åè¿ç§»åˆ°æ­£å¼æ¨¡å—

ä½¿ç”¨ç¤ºä¾‹ï¼š
==========
from factors.generator.financial.experimental_factors import ExperimentalFactorCalculator

calculator = ExperimentalFactorCalculator()
# ç›´æ¥è°ƒç”¨æµ‹è¯•
new_factor = calculator.calculate_EXPERIMENTAL_YourIdea_ttm(financial_data)
# å¿«é€ŸéªŒè¯
result = calculator.quick_validate_factor(new_factor, 'YourIdea')
"""

import pandas as pd
import numpy as np
from typing import Optional, Dict, List, Union, Any, Tuple
import logging
from datetime import datetime
import warnings
from scipy import stats
from collections import defaultdict

from .pure_financial_factors import PureFinancialFactorCalculator
from ...base.time_series_processor import TimeSeriesProcessor
from ...config.field_mapper import get_field_mapper

logger = logging.getLogger(__name__)


class ExperimentalFactorCalculator(PureFinancialFactorCalculator):
    """
    å®éªŒæ€§å› å­è®¡ç®—å™¨
    
    ç»§æ‰¿PureFinancialFactorCalculatorçš„æ‰€æœ‰åŠŸèƒ½ï¼Œä¸“é—¨ç”¨äºæµ‹è¯•æ–°å› å­æƒ³æ³•
    """
    
    def __init__(self):
        super().__init__()
        self.description = "Experimental Factor Calculator - For testing new factor ideas"
        
        # å®éªŒæ€§å› å­ä¸ä¼šæ³¨å†Œåˆ°æ­£å¼ç³»ç»Ÿ
        self.experimental_factors = []
        self.validation_results = {}
        
        # åŠ è½½å­—æ®µæ˜ å°„å™¨
        self.field_mapper = get_field_mapper()
        
        logger.info("ğŸ§ª å®éªŒæ€§å› å­è®¡ç®—å™¨å·²åˆå§‹åŒ–")
    
    # =====================================================
    # å®éªŒæ€§å› å­æ¨¡æ¿å’Œç¤ºä¾‹
    # =====================================================
    
    def calculate_EXPERIMENTAL_Template_ttm(self, financial_data: pd.DataFrame, **kwargs) -> pd.Series:
        """
        å®éªŒæ€§å› å­æ¨¡æ¿ - å¤åˆ¶è¿™ä¸ªæ¨¡æ¿å¼€å§‹æ–°å› å­å¼€å‘
        
        è®¡ç®—å…¬å¼ï¼š[åœ¨è¿™é‡Œæè¿°ä½ çš„å› å­è®¡ç®—å…¬å¼]
        ç»æµå«ä¹‰ï¼š[è§£é‡Šè¿™ä¸ªå› å­çš„ç»æµå­¦æ„ä¹‰]
        å‡è®¾éªŒè¯ï¼š[è¯´æ˜ä½ æƒ³éªŒè¯çš„å‡è®¾]
        
        Parameters:
        -----------
        financial_data : pd.DataFrame
            è´¢åŠ¡æ•°æ®
        **kwargs : dict
            å…¶ä»–å‚æ•°
            
        Returns:
        --------
        pd.Series
            å› å­å€¼
        """
        # æ­¥éª¤1ï¼šéªŒè¯æ•°æ®éœ€æ±‚
        required_cols = ['DEDUCTEDPROFIT', 'FIN_EXP_IS', 'TAX', 'd_quarter']  # ä¿®æ”¹ä¸ºä½ éœ€è¦çš„åˆ—
        if not self.validate_data_requirements(financial_data, required_cols):
            raise ValueError(f"Required data not available for Template calculation")
        
        # æ­¥éª¤2ï¼šæå–æ•°æ®
        extracted_data = self.extract_required_data(financial_data, required_cols)
         
        # æ­¥éª¤3ï¼šå®ç°ä½ çš„è®¡ç®—é€»è¾‘
        # TODO: åœ¨è¿™é‡Œå®ç°å…·ä½“çš„å› å­è®¡ç®—
        result = pd.Series(index=financial_data.index, dtype=float)
        
        # æ­¥éª¤4ï¼šè¿”å›ç»“æœ
        logger.info("âœ¨ å®éªŒæ€§å› å­Templateè®¡ç®—å®Œæˆ")
        return result
    
    def calculate_EXPERIMENTAL_ProfitGrowthQuality_ttm(self, financial_data: pd.DataFrame, **kwargs) -> pd.Series:
        """
        å®éªŒæ€§å› å­ï¼šç›ˆåˆ©å¢é•¿è´¨é‡ - ç¤ºä¾‹å®ç°
        
        è®¡ç®—å…¬å¼ï¼š(TTMå‡€åˆ©æ¶¦å¢é•¿ç‡ Ã— ç»è¥ç°é‡‘æµ/å‡€åˆ©æ¶¦) / ROEæ³¢åŠ¨ç‡
        ç»æµå«ä¹‰ï¼šè¡¡é‡ä¼ä¸šç›ˆåˆ©å¢é•¿çš„è´¨é‡å’Œå¯æŒç»­æ€§
        å‡è®¾éªŒè¯ï¼šé«˜è´¨é‡çš„ç›ˆåˆ©å¢é•¿åº”è¯¥ä¼´éšç°é‡‘æµå¢é•¿ä¸”ROEç¨³å®š
        
        Parameters:
        -----------
        financial_data : pd.DataFrame
            è´¢åŠ¡æ•°æ®ï¼Œéœ€åŒ…å«å‡€åˆ©æ¶¦ã€ç»è¥ç°é‡‘æµç­‰å­—æ®µ
        **kwargs : dict
            å…¶ä»–å‚æ•°
            
        Returns:
        --------
        pd.Series
            ç›ˆåˆ©å¢é•¿è´¨é‡å› å­å€¼
        """
        try:
            # éªŒè¯æ•°æ®éœ€æ±‚
            required_cols = ['earnings', 'operating_cash_flow', 'equity', 'quarter']
            if not self.validate_data_requirements(financial_data, required_cols):
                raise ValueError("Required data not available for ProfitGrowthQuality calculation")
            
            extracted_data = self.extract_required_data(financial_data, required_cols)
            
            # 1. è®¡ç®—TTMå‡€åˆ©æ¶¦
            earnings_data = extracted_data[['earnings', 'quarter']].copy()
            earnings_data = earnings_data.rename(columns={
                'earnings': 'DEDUCTEDPROFIT', 
                'quarter': 'd_quarter'
            })
            earnings_ttm = TimeSeriesProcessor.calculate_ttm(earnings_data)
            
            # 2. è®¡ç®—TTMç»è¥ç°é‡‘æµ
            cf_data = extracted_data[['operating_cash_flow', 'quarter']].copy()
            cf_data = cf_data.rename(columns={
                'operating_cash_flow': 'NETCASH_OPER', 
                'quarter': 'd_quarter'
            })
            cf_ttm = TimeSeriesProcessor.calculate_ttm(cf_data)
            
            # 3. è®¡ç®—ROE
            roe_data = extracted_data[['earnings', 'equity', 'quarter']].copy()
            roe_data = roe_data.rename(columns={
                'earnings': 'DEDUCTEDPROFIT',
                'equity': 'EQY_BELONGTO_PARCOMSH',
                'quarter': 'd_quarter'
            })
            roe_ttm_data = TimeSeriesProcessor.calculate_ttm(roe_data[['DEDUCTEDPROFIT', 'd_quarter']])
            equity_avg = TimeSeriesProcessor.calculate_avg(roe_data[['EQY_BELONGTO_PARCOMSH']])
            
            # 4. è®¡ç®—å„ä¸ªç»„ä»¶
            earnings_series = earnings_ttm.iloc[:, 0] if earnings_ttm.shape[1] > 0 else pd.Series(dtype=float)
            cf_series = cf_ttm.iloc[:, 0] if cf_ttm.shape[1] > 0 else pd.Series(dtype=float)
            roe_ttm_series = roe_ttm_data.iloc[:, 0] if roe_ttm_data.shape[1] > 0 else pd.Series(dtype=float)
            equity_series = equity_avg.iloc[:, 0] if equity_avg.shape[1] > 0 else pd.Series(dtype=float)
            
            # å¯¹é½æ•°æ®
            earnings_aligned, cf_aligned = earnings_series.align(cf_series, join='inner')
            roe_values = self._safe_division(roe_ttm_series, equity_series)
            
            # 5. è®¡ç®—å‡€åˆ©æ¶¦åŒæ¯”å¢é•¿ç‡
            earnings_yoy = TimeSeriesProcessor.calculate_yoy(earnings_ttm)
            earnings_growth = earnings_yoy.iloc[:, 0] if earnings_yoy.shape[1] > 0 else pd.Series(dtype=float)
            
            # 6. è®¡ç®—ç°é‡‘æµè´¨é‡ (ç»è¥ç°é‡‘æµ/å‡€åˆ©æ¶¦)
            cf_quality = self._safe_division(cf_aligned, earnings_aligned)
            
            # 7. è®¡ç®—ROEæ³¢åŠ¨ç‡ (è¿‡å»8ä¸ªå­£åº¦çš„æ ‡å‡†å·®)
            roe_volatility = roe_values.groupby(level='StockCodes').rolling(window=8, min_periods=4).std()
            if isinstance(roe_volatility.index, pd.MultiIndex):
                roe_volatility.index = roe_volatility.index.droplevel(0)
            
            # 8. ç»¼åˆè®¡ç®—ç›ˆåˆ©å¢é•¿è´¨é‡
            # (å‡€åˆ©æ¶¦å¢é•¿ç‡ Ã— ç°é‡‘æµè´¨é‡) / ROEæ³¢åŠ¨ç‡
            growth_quality = self._safe_division(
                earnings_growth * cf_quality,
                roe_volatility
            )
            
            # æ¸…ç†å¼‚å¸¸å€¼
            growth_quality = growth_quality.replace([np.inf, -np.inf], np.nan)
            
            logger.info("âœ¨ å®éªŒæ€§å› å­ProfitGrowthQualityè®¡ç®—å®Œæˆ")
            return growth_quality
            
        except Exception as e:
            logger.error(f"ProfitGrowthQualityè®¡ç®—å¤±è´¥: {e}")
            return pd.Series(index=financial_data.index, dtype=float)
    
    def calculate_EXPERIMENTAL_DebtServiceAbility_ttm(self, financial_data: pd.DataFrame, **kwargs) -> pd.Series:
        """
        å®éªŒæ€§å› å­ï¼šå€ºåŠ¡å¿ä»˜èƒ½åŠ› - ç¤ºä¾‹å®ç°
        
        è®¡ç®—å…¬å¼ï¼š(ç»è¥ç°é‡‘æµTTM + è´§å¸èµ„é‡‘) / (çŸ­æœŸå€Ÿæ¬¾ + ä¸€å¹´å†…åˆ°æœŸçš„é•¿æœŸè´Ÿå€º + åˆ©æ¯è´¹ç”¨TTM)
        ç»æµå«ä¹‰ï¼šè¡¡é‡ä¼ä¸šçŸ­æœŸå¿å€ºèƒ½åŠ›ï¼Œè€ƒè™‘ç°é‡‘æµå’Œç°æœ‰èµ„é‡‘
        å‡è®¾éªŒè¯ï¼šæ›´å¥½çš„å€ºåŠ¡å¿ä»˜èƒ½åŠ›åº”è¯¥é™ä½è¿çº¦é£é™©ï¼Œæå‡ä¼°å€¼
        """
        try:
            required_cols = ['operating_cash_flow', 'cash_equivalents', 'short_term_debt', 'financial_expense', 'quarter']
            if not self.validate_data_requirements(financial_data, required_cols):
                logger.warning("Data not sufficient for DebtServiceAbility calculation")
                return pd.Series(index=financial_data.index, dtype=float)
            
            extracted_data = self.extract_required_data(financial_data, required_cols)
            
            # ç»è¥ç°é‡‘æµTTM
            cf_data = extracted_data[['operating_cash_flow', 'quarter']].copy()
            cf_data = cf_data.rename(columns={'operating_cash_flow': 'NETCASH_OPER', 'quarter': 'd_quarter'})
            cf_ttm = TimeSeriesProcessor.calculate_ttm(cf_data)
            
            # åˆ©æ¯è´¹ç”¨TTM
            interest_data = extracted_data[['financial_expense', 'quarter']].copy()
            interest_data = interest_data.rename(columns={'financial_expense': 'FIN_EXP_IS', 'quarter': 'd_quarter'})
            interest_ttm = TimeSeriesProcessor.calculate_ttm(interest_data)
            
            # è·å–æ•°å€¼
            cf_values = cf_ttm.iloc[:, 0] if cf_ttm.shape[1] > 0 else pd.Series(dtype=float)
            interest_values = interest_ttm.iloc[:, 0] if interest_ttm.shape[1] > 0 else pd.Series(dtype=float)
            cash_values = extracted_data['cash_equivalents']
            debt_values = extracted_data['short_term_debt']
            
            # è®¡ç®—å¿å€ºèƒ½åŠ› = (ç°é‡‘æµ + ç°é‡‘) / (çŸ­æœŸå€ºåŠ¡ + åˆ©æ¯)
            numerator = cf_values + cash_values
            denominator = debt_values + interest_values.abs()  # åˆ©æ¯è´¹ç”¨å–ç»å¯¹å€¼
            
            debt_service_ability = self._safe_division(numerator, denominator)
            
            logger.info("âœ¨ å®éªŒæ€§å› å­DebtServiceAbilityè®¡ç®—å®Œæˆ")
            return debt_service_ability
            
        except Exception as e:
            logger.error(f"DebtServiceAbilityè®¡ç®—å¤±è´¥: {e}")
            return pd.Series(index=financial_data.index, dtype=float)
    
    # =====================================================
    # å•å› å­æ£€éªŒæ¨¡å—
    # =====================================================
    
    def single_factor_test(self,
                          factor_data: pd.Series,
                          return_data: pd.Series,
                          factor_name: str,
                          periods: List[int] = [1, 5, 10, 20],
                          quantiles: int = 5,
                          save_results: bool = True) -> Dict[str, Any]:
        """
        å®Œæ•´çš„å•å› å­æ£€éªŒ
        
        Parameters:
        -----------
        factor_data : å› å­æ•°æ® (MultiIndex: TradingDates, StockCodes)
        return_data : æ”¶ç›Šç‡æ•°æ® (MultiIndex: TradingDates, StockCodes)
        factor_name : å› å­åç§°
        periods : æµ‹è¯•çš„æŒæœ‰æœŸåˆ—è¡¨
        quantiles : åˆ†ç»„æ•°é‡
        save_results : æ˜¯å¦ä¿å­˜ç»“æœ
        
        Returns:
        --------
        å®Œæ•´æ£€éªŒç»“æœå­—å…¸
        """
        logger.info(f"ğŸ”¬ å¼€å§‹å•å› å­æ£€éªŒ: {factor_name}")
        
        test_results = {
            'factor_name': factor_name,
            'test_time': datetime.now(),
            'ic_analysis': {},
            'group_analysis': {},
            'monotonicity_test': {},
            'significance_test': {},
            'decay_analysis': {},
            'summary': {}
        }
        
        try:
            # 1. ICåˆ†æ
            logger.info("   ğŸ“Š æ‰§è¡ŒICåˆ†æ...")
            test_results['ic_analysis'] = self._calculate_ic_analysis(
                factor_data, return_data, periods
            )
            
            # 2. åˆ†ç»„åˆ†æ
            logger.info("   ğŸ“ˆ æ‰§è¡Œåˆ†ç»„åˆ†æ...")
            test_results['group_analysis'] = self._calculate_group_analysis(
                factor_data, return_data, periods, quantiles
            )
            
            # 3. å•è°ƒæ€§æ£€éªŒ
            logger.info("   ğŸ“‰ æ‰§è¡Œå•è°ƒæ€§æ£€éªŒ...")
            test_results['monotonicity_test'] = self._test_monotonicity(
                test_results['group_analysis']
            )
            
            # 4. ç»Ÿè®¡æ˜¾è‘—æ€§æ£€éªŒ
            logger.info("   ğŸ¯ æ‰§è¡Œæ˜¾è‘—æ€§æ£€éªŒ...")
            test_results['significance_test'] = self._test_significance(
                test_results['ic_analysis'], test_results['group_analysis']
            )
            
            # 5. å› å­è¡°å‡åˆ†æ
            logger.info("   â° æ‰§è¡Œè¡°å‡åˆ†æ...")
            test_results['decay_analysis'] = self._analyze_factor_decay(
                factor_data, return_data, periods=[1, 5, 10, 20, 60]
            )
            
            # 6. ç”Ÿæˆç»¼åˆè¯„ä»·
            logger.info("   ğŸ“‹ ç”Ÿæˆç»¼åˆè¯„ä»·...")
            test_results['summary'] = self._generate_test_summary(test_results)
            
            # ä¿å­˜ç»“æœ
            if save_results:
                self.validation_results[f"{factor_name}_single_test"] = test_results
            
            # æ‰“å°æŠ¥å‘Š
            self._print_single_factor_report(test_results)
            
        except Exception as e:
            logger.error(f"å•å› å­æ£€éªŒå¤±è´¥: {e}")
            test_results['error'] = str(e)
        
        return test_results
    
    def _calculate_ic_analysis(self, 
                              factor_data: pd.Series,
                              return_data: pd.Series,
                              periods: List[int]) -> Dict[str, Any]:
        """è®¡ç®—ICåˆ†æ"""
        ic_results = {}
        
        # ç¡®ä¿æ•°æ®å¯¹é½
        factor_aligned, return_aligned = factor_data.align(return_data, join='inner')
        
        if len(factor_aligned) == 0:
            return {'error': 'No aligned data for IC calculation'}
        
        for period in periods:
            # è®¡ç®—å‰ç»æ”¶ç›Š
            future_returns = self._calculate_forward_returns(return_aligned, period)
            
            # å¯¹é½å› å­å€¼å’Œå‰ç»æ”¶ç›Š
            factor_for_ic, returns_for_ic = factor_aligned.align(future_returns, join='inner')
            
            if len(factor_for_ic) == 0:
                ic_results[f'period_{period}'] = {'error': 'No data for this period'}
                continue
            
            # æŒ‰æ—¥æœŸåˆ†ç»„è®¡ç®—IC
            dates = factor_for_ic.index.get_level_values(0).unique()
            daily_ics = []
            
            for date in dates:
                date_factor = factor_for_ic[factor_for_ic.index.get_level_values(0) == date]
                date_return = returns_for_ic[returns_for_ic.index.get_level_values(0) == date]
                
                if len(date_factor) > 10:  # è‡³å°‘10åªè‚¡ç¥¨
                    # è®¡ç®—Spearmanç›¸å…³ç³»æ•°ï¼ˆRank ICï¼‰
                    ic_value = stats.spearmanr(date_factor, date_return)[0]
                    if not np.isnan(ic_value):
                        daily_ics.append(ic_value)
            
            if len(daily_ics) > 0:
                daily_ics = np.array(daily_ics)
                
                ic_results[f'period_{period}'] = {
                    'mean_ic': np.mean(daily_ics),
                    'std_ic': np.std(daily_ics),
                    'ic_ir': np.mean(daily_ics) / np.std(daily_ics) if np.std(daily_ics) > 0 else 0,
                    'win_rate': np.sum(daily_ics > 0) / len(daily_ics),
                    'daily_ics': daily_ics.tolist(),
                    'ic_t_stat': stats.ttest_1samp(daily_ics, 0)[0],
                    'ic_p_value': stats.ttest_1samp(daily_ics, 0)[1]
                }
            else:
                ic_results[f'period_{period}'] = {'error': 'Insufficient data for IC calculation'}
        
        return ic_results
    
    def _calculate_forward_returns(self, 
                                  return_data: pd.Series, 
                                  period: int) -> pd.Series:
        """
        è®¡ç®—å‰ç»æ”¶ç›Š
        
        Parameters:
        -----------
        return_data : pd.Series
            æ—¥æ”¶ç›Šç‡æ•°æ®ï¼ˆé€šå¸¸æ˜¯å¯¹æ•°æ”¶ç›Šç‡ï¼‰
        period : int
            æŒæœ‰æœŸå¤©æ•°
            
        Returns:
        --------
        pd.Series
            å‰ç»æ”¶ç›Šç‡
            - period=1: ç›´æ¥ä½¿ç”¨ä¸‹ä¸€æœŸæ”¶ç›Šç‡
            - period>1: ç´¯ç§¯å¯¹æ•°æ”¶ç›Šç‡ï¼ˆé€‚ç”¨äºæ—¥æ”¶ç›Šç‡æ˜¯å¯¹æ•°æ”¶ç›Šç‡çš„æƒ…å†µï¼‰
        """
        def calc_forward_return(stock_returns):
            if period == 1:
                # å•æœŸï¼šç›´æ¥ä½¿ç”¨ä¸‹ä¸€æœŸæ”¶ç›Šç‡
                return stock_returns.shift(-1)
            else:
                # å¤šæœŸï¼šç´¯ç§¯å¯¹æ•°æ”¶ç›Šç‡
                # ä½¿ç”¨rollingçª—å£å‘å‰è®¡ç®—ç´¯ç§¯æ”¶ç›Š
                forward_cumulative = stock_returns.rolling(
                    window=period, 
                    min_periods=period
                ).sum().shift(-period)
                return forward_cumulative
        
        forward_returns = return_data.groupby(level='StockCodes').apply(calc_forward_return)
        
        # é‡æ–°æ•´ç†ç´¢å¼•
        if isinstance(forward_returns.index, pd.MultiIndex):
            forward_returns.index = forward_returns.index.droplevel(0)
        
        return forward_returns
    
    def _calculate_group_analysis(self,
                                 factor_data: pd.Series,
                                 return_data: pd.Series,
                                 periods: List[int],
                                 quantiles: int) -> Dict[str, Any]:
        """è®¡ç®—åˆ†ç»„åˆ†æ"""
        group_results = {}
        
        # ç¡®ä¿æ•°æ®å¯¹é½
        factor_aligned, return_aligned = factor_data.align(return_data, join='inner')
        
        for period in periods:
            # è®¡ç®—å‰ç»æ”¶ç›Š
            future_returns = self._calculate_forward_returns(return_aligned, period)
            
            # å¯¹é½æ•°æ®
            factor_for_group, returns_for_group = factor_aligned.align(future_returns, join='inner')
            
            if len(factor_for_group) == 0:
                group_results[f'period_{period}'] = {'error': 'No data for grouping'}
                continue
            
            # æŒ‰æ—¥æœŸåˆ†ç»„åˆ†æ
            dates = factor_for_group.index.get_level_values(0).unique()
            daily_group_returns = []
            
            for date in dates:
                date_factor = factor_for_group[factor_for_group.index.get_level_values(0) == date]
                date_return = returns_for_group[returns_for_group.index.get_level_values(0) == date]
                
                if len(date_factor) >= quantiles * 5:  # ç¡®ä¿æ¯ç»„è‡³å°‘5åªè‚¡ç¥¨
                    # æŒ‰å› å­å€¼åˆ†ç»„
                    factor_ranks = date_factor.rank(pct=True)
                    
                    group_returns = []
                    for q in range(quantiles):
                        q_min = q / quantiles
                        q_max = (q + 1) / quantiles
                        
                        group_mask = (factor_ranks >= q_min) & (factor_ranks < q_max)
                        if q == quantiles - 1:  # æœ€åä¸€ç»„åŒ…å«ç­‰äºä¸Šç•Œçš„å€¼
                            group_mask = (factor_ranks >= q_min) & (factor_ranks <= q_max)
                        
                        group_stocks = date_factor[group_mask]
                        if len(group_stocks) > 0:
                            group_return = date_return[group_stocks.index].mean()
                            group_returns.append(group_return)
                        else:
                            group_returns.append(np.nan)
                    
                    if not all(np.isnan(group_returns)):
                        daily_group_returns.append(group_returns)
            
            if len(daily_group_returns) > 0:
                # è®¡ç®—å„ç»„å¹³å‡æ”¶ç›Š
                daily_group_returns = np.array(daily_group_returns)
                
                group_stats = {}
                for q in range(quantiles):
                    group_q_returns = daily_group_returns[:, q]
                    valid_returns = group_q_returns[~np.isnan(group_q_returns)]
                    
                    if len(valid_returns) > 0:
                        group_stats[f'group_{q+1}'] = {
                            'mean_return': np.mean(valid_returns),
                            'std_return': np.std(valid_returns),
                            'sharpe_ratio': np.mean(valid_returns) / np.std(valid_returns) if np.std(valid_returns) > 0 else 0,
                            'win_rate': np.sum(valid_returns > 0) / len(valid_returns),
                            'daily_returns': valid_returns.tolist()
                        }
                
                # è®¡ç®—å¤šç©ºæ”¶ç›Š (æœ€é«˜ç»„ - æœ€ä½ç»„)
                if f'group_{quantiles}' in group_stats and 'group_1' in group_stats:
                    high_returns = np.array(group_stats[f'group_{quantiles}']['daily_returns'])
                    low_returns = np.array(group_stats['group_1']['daily_returns'])
                    
                    # ç¡®ä¿é•¿åº¦ä¸€è‡´
                    min_len = min(len(high_returns), len(low_returns))
                    if min_len > 0:
                        long_short = high_returns[:min_len] - low_returns[:min_len]
                        
                        group_stats['long_short'] = {
                            'mean_return': np.mean(long_short),
                            'std_return': np.std(long_short),
                            'sharpe_ratio': np.mean(long_short) / np.std(long_short) if np.std(long_short) > 0 else 0,
                            'win_rate': np.sum(long_short > 0) / len(long_short),
                            't_stat': stats.ttest_1samp(long_short, 0)[0],
                            'p_value': stats.ttest_1samp(long_short, 0)[1]
                        }
                
                group_results[f'period_{period}'] = group_stats
            else:
                group_results[f'period_{period}'] = {'error': 'No valid group data'}
        
        return group_results
    
    def _test_monotonicity(self, group_analysis: Dict[str, Any]) -> Dict[str, Any]:
        """æ£€éªŒå› å­å•è°ƒæ€§"""
        monotonicity_results = {}
        
        for period, period_data in group_analysis.items():
            if 'error' in period_data:
                monotonicity_results[period] = {'error': period_data['error']}
                continue
            
            # æå–å„ç»„æ”¶ç›Šç‡
            group_returns = []
            for key in sorted(period_data.keys()):
                if key.startswith('group_') and not key == 'long_short':
                    group_returns.append(period_data[key]['mean_return'])
            
            if len(group_returns) >= 3:
                # è®¡ç®—å•è°ƒæ€§æŒ‡æ ‡
                # 1. Spearman rank correlation
                group_ranks = list(range(1, len(group_returns) + 1))
                monotonicity_corr = stats.spearmanr(group_ranks, group_returns)[0]
                
                # 2. å•è°ƒé€’å¢çš„ç»„æ•°
                increasing_pairs = 0
                total_pairs = 0
                for i in range(len(group_returns)):
                    for j in range(i + 1, len(group_returns)):
                        total_pairs += 1
                        if group_returns[j] > group_returns[i]:
                            increasing_pairs += 1
                
                monotonicity_rate = increasing_pairs / total_pairs if total_pairs > 0 else 0
                
                monotonicity_results[period] = {
                    'monotonicity_corr': monotonicity_corr,
                    'monotonicity_rate': monotonicity_rate,
                    'group_returns': group_returns,
                    'is_monotonic': monotonicity_rate > 0.7  # 70%ä»¥ä¸Šçš„ç»„å¯¹æ˜¯é€’å¢çš„
                }
            else:
                monotonicity_results[period] = {'error': 'Insufficient groups for monotonicity test'}
        
        return monotonicity_results
    
    def _test_significance(self, 
                          ic_analysis: Dict[str, Any],
                          group_analysis: Dict[str, Any]) -> Dict[str, Any]:
        """ç»Ÿè®¡æ˜¾è‘—æ€§æ£€éªŒ"""
        significance_results = {}
        
        for period in ic_analysis.keys():
            if period in group_analysis:
                sig_results = {}
                
                # ICæ˜¾è‘—æ€§
                ic_data = ic_analysis[period]
                if 'ic_p_value' in ic_data:
                    sig_results['ic_significant'] = ic_data['ic_p_value'] < 0.05
                    sig_results['ic_p_value'] = ic_data['ic_p_value']
                    sig_results['ic_t_stat'] = ic_data['ic_t_stat']
                
                # å¤šç©ºæ”¶ç›Šæ˜¾è‘—æ€§
                group_data = group_analysis[period]
                if 'long_short' in group_data:
                    ls_data = group_data['long_short']
                    sig_results['long_short_significant'] = ls_data['p_value'] < 0.05
                    sig_results['long_short_p_value'] = ls_data['p_value']
                    sig_results['long_short_t_stat'] = ls_data['t_stat']
                
                significance_results[period] = sig_results
        
        return significance_results
    
    def _analyze_factor_decay(self,
                             factor_data: pd.Series,
                             return_data: pd.Series,
                             periods: List[int]) -> Dict[str, Any]:
        """å› å­è¡°å‡åˆ†æ"""
        decay_results = {}
        
        try:
            # è®¡ç®—ä¸åŒæœŸé—´çš„IC
            ic_by_period = {}
            for period in periods:
                ic_result = self._calculate_ic_analysis(factor_data, return_data, [period])
                if f'period_{period}' in ic_result and 'mean_ic' in ic_result[f'period_{period}']:
                    ic_by_period[period] = ic_result[f'period_{period}']['mean_ic']
            
            if len(ic_by_period) > 0:
                # è®¡ç®—è¡°å‡ç‡
                periods_sorted = sorted(ic_by_period.keys())
                ic_values = [ic_by_period[p] for p in periods_sorted]
                
                # è®¡ç®—ç›¸å¯¹äºç¬¬ä¸€æœŸçš„è¡°å‡
                if len(ic_values) > 1 and abs(ic_values[0]) > 0.001:
                    decay_rates = [(ic_values[i] / ic_values[0] - 1) for i in range(1, len(ic_values))]
                    
                    decay_results = {
                        'periods': periods_sorted,
                        'ic_values': ic_values,
                        'decay_rates': decay_rates,
                        'half_life_period': None
                    }
                    
                    # ä¼°ç®—åŠè¡°æœŸ
                    half_ic = ic_values[0] * 0.5
                    for i, ic in enumerate(ic_values[1:], 1):
                        if abs(ic) <= abs(half_ic):
                            decay_results['half_life_period'] = periods_sorted[i]
                            break
                
        except Exception as e:
            decay_results = {'error': str(e)}
        
        return decay_results
    
    def _generate_test_summary(self, test_results: Dict[str, Any]) -> Dict[str, Any]:
        """ç”Ÿæˆç»¼åˆè¯„ä»·"""
        summary = {
            'overall_score': 0,
            'grade': 'F',
            'strengths': [],
            'weaknesses': [],
            'recommendations': []
        }
        
        try:
            score = 0
            total_checks = 0
            
            # 1. ICåˆ†æè¯„åˆ† (30åˆ†)
            ic_scores = []
            for period, ic_data in test_results['ic_analysis'].items():
                if 'mean_ic' in ic_data:
                    abs_ic = abs(ic_data['mean_ic'])
                    ic_ir = ic_data.get('ic_ir', 0)
                    
                    if abs_ic > 0.05:
                        ic_scores.append(15)
                    elif abs_ic > 0.03:
                        ic_scores.append(10)
                    elif abs_ic > 0.01:
                        ic_scores.append(5)
                    else:
                        ic_scores.append(0)
                    
                    if abs(ic_ir) > 1.0:
                        ic_scores[-1] += 15
                    elif abs(ic_ir) > 0.5:
                        ic_scores[-1] += 10
                    else:
                        ic_scores[-1] += 5
            
            if ic_scores:
                score += max(ic_scores)
                total_checks += 30
                if max(ic_scores) > 20:
                    summary['strengths'].append('ICè´¨é‡ä¼˜ç§€')
                elif max(ic_scores) < 10:
                    summary['weaknesses'].append('ICè¾ƒå¼±')
            
            # 2. åˆ†ç»„åˆ†æè¯„åˆ† (25åˆ†)
            group_scores = []
            for period, group_data in test_results['group_analysis'].items():
                if 'long_short' in group_data:
                    ls_return = group_data['long_short']['mean_return']
                    ls_sharpe = group_data['long_short']['sharpe_ratio']
                    
                    if abs(ls_return) > 0.01:
                        group_scores.append(15)
                    elif abs(ls_return) > 0.005:
                        group_scores.append(10)
                    else:
                        group_scores.append(5)
                    
                    if abs(ls_sharpe) > 1.0:
                        group_scores[-1] += 10
                    elif abs(ls_sharpe) > 0.5:
                        group_scores[-1] += 5
            
            if group_scores:
                score += max(group_scores)
                total_checks += 25
                if max(group_scores) > 20:
                    summary['strengths'].append('åˆ†ç»„æ•ˆæœæ˜¾è‘—')
            
            # 3. å•è°ƒæ€§è¯„åˆ† (20åˆ†)
            mono_scores = []
            for period, mono_data in test_results['monotonicity_test'].items():
                if 'monotonicity_rate' in mono_data:
                    mono_rate = mono_data['monotonicity_rate']
                    if mono_rate > 0.8:
                        mono_scores.append(20)
                    elif mono_rate > 0.6:
                        mono_scores.append(15)
                    elif mono_rate > 0.4:
                        mono_scores.append(10)
                    else:
                        mono_scores.append(5)
            
            if mono_scores:
                score += max(mono_scores)
                total_checks += 20
                if max(mono_scores) > 15:
                    summary['strengths'].append('å•è°ƒæ€§è‰¯å¥½')
                else:
                    summary['weaknesses'].append('å•è°ƒæ€§è¾ƒå·®')
            
            # 4. æ˜¾è‘—æ€§è¯„åˆ† (15åˆ†)
            sig_count = 0
            sig_total = 0
            for period, sig_data in test_results['significance_test'].items():
                if 'ic_significant' in sig_data:
                    sig_total += 1
                    if sig_data['ic_significant']:
                        sig_count += 1
                if 'long_short_significant' in sig_data:
                    sig_total += 1
                    if sig_data['long_short_significant']:
                        sig_count += 1
            
            if sig_total > 0:
                sig_score = (sig_count / sig_total) * 15
                score += sig_score
                total_checks += 15
                if sig_score > 10:
                    summary['strengths'].append('ç»Ÿè®¡æ˜¾è‘—æ€§å¼º')
            
            # 5. è¡°å‡åˆ†æè¯„åˆ† (10åˆ†)
            if 'half_life_period' in test_results['decay_analysis']:
                half_life = test_results['decay_analysis']['half_life_period']
                if half_life and half_life > 10:
                    score += 10
                    summary['strengths'].append('å› å­æŒç»­æ€§å¥½')
                elif half_life and half_life > 5:
                    score += 7
                else:
                    score += 3
                    summary['weaknesses'].append('å› å­è¡°å‡è¾ƒå¿«')
                total_checks += 10
            
            # è®¡ç®—æœ€ç»ˆè¯„åˆ†
            if total_checks > 0:
                summary['overall_score'] = (score / total_checks) * 100
            
            # è¯„çº§
            if summary['overall_score'] >= 80:
                summary['grade'] = 'A'
                summary['recommendations'].append('ğŸ‰ ä¼˜ç§€å› å­ï¼Œå¼ºçƒˆæ¨èè¿›å…¥ç”Ÿäº§ç¯å¢ƒ')
            elif summary['overall_score'] >= 70:
                summary['grade'] = 'B'
                summary['recommendations'].append('ğŸŒŸ è‰¯å¥½å› å­ï¼Œå»ºè®®è¿›ä¸€æ­¥ä¼˜åŒ–åä½¿ç”¨')
            elif summary['overall_score'] >= 60:
                summary['grade'] = 'C'
                summary['recommendations'].append('âš¡ ä¸€èˆ¬å› å­ï¼Œéœ€è¦æ˜¾è‘—æ”¹è¿›')
            elif summary['overall_score'] >= 40:
                summary['grade'] = 'D'
                summary['recommendations'].append('âš ï¸ è¾ƒå·®å› å­ï¼Œå»ºè®®é‡æ–°è®¾è®¡')
            else:
                summary['grade'] = 'F'
                summary['recommendations'].append('âŒ æ— æ•ˆå› å­ï¼Œä¸å»ºè®®ä½¿ç”¨')
        
        except Exception as e:
            summary['error'] = str(e)
        
        return summary
    
    def _print_single_factor_report(self, test_results: Dict[str, Any]):
        """æ‰“å°å•å› å­æ£€éªŒæŠ¥å‘Š"""
        print("\n" + "=" * 80)
        print(f"ğŸ”¬ å•å› å­æ£€éªŒæŠ¥å‘Š: {test_results['factor_name']}")
        print("=" * 80)
        
        # ICåˆ†æç»“æœ
        print(f"\nğŸ“Š ICåˆ†æ:")
        for period, ic_data in test_results['ic_analysis'].items():
            if 'mean_ic' in ic_data:
                period_num = period.split('_')[1]
                print(f"   {period_num}æœŸ - IC: {ic_data['mean_ic']:.4f}, "
                      f"IR: {ic_data['ic_ir']:.4f}, "
                      f"èƒœç‡: {ic_data['win_rate']:.2%}, "
                      f"tå€¼: {ic_data['ic_t_stat']:.2f}")
        
        # åˆ†ç»„åˆ†æç»“æœ
        print(f"\nğŸ“ˆ åˆ†ç»„åˆ†æ:")
        for period, group_data in test_results['group_analysis'].items():
            if 'long_short' in group_data:
                period_num = period.split('_')[1]
                ls_data = group_data['long_short']
                print(f"   {period_num}æœŸå¤šç©º - æ”¶ç›Š: {ls_data['mean_return']:.4f}, "
                      f"å¤æ™®: {ls_data['sharpe_ratio']:.4f}, "
                      f"èƒœç‡: {ls_data['win_rate']:.2%}, "
                      f"tå€¼: {ls_data['t_stat']:.2f}")
        
        # å•è°ƒæ€§æ£€éªŒ
        print(f"\nğŸ“‰ å•è°ƒæ€§æ£€éªŒ:")
        for period, mono_data in test_results['monotonicity_test'].items():
            if 'monotonicity_rate' in mono_data:
                period_num = period.split('_')[1]
                print(f"   {period_num}æœŸ - å•è°ƒæ€§: {mono_data['monotonicity_rate']:.2%}, "
                      f"ç›¸å…³æ€§: {mono_data['monotonicity_corr']:.4f}")
        
        # ç»¼åˆè¯„ä»·
        summary = test_results['summary']
        print(f"\nâ­ ç»¼åˆè¯„ä»·:")
        print(f"   è¯„åˆ†: {summary['overall_score']:.1f}/100 (ç­‰çº§: {summary['grade']})")
        
        if summary['strengths']:
            print(f"   ä¼˜åŠ¿: {', '.join(summary['strengths'])}")
        if summary['weaknesses']:
            print(f"   åŠ£åŠ¿: {', '.join(summary['weaknesses'])}")
        if summary['recommendations']:
            print(f"   å»ºè®®: {summary['recommendations'][0]}")
        
        print("=" * 80)
    
    # =====================================================
    # å¿«é€ŸéªŒè¯å’Œæµ‹è¯•å·¥å…·
    # =====================================================
    
    def quick_validate_factor(self, 
                             factor_data: pd.Series,
                             factor_name: str,
                             save_results: bool = True) -> Dict[str, Any]:
        """
        å¿«é€ŸéªŒè¯å®éªŒæ€§å› å­çš„åŸºæœ¬ç‰¹å¾
        
        Parameters:
        -----------
        factor_data : å› å­æ•°æ®
        factor_name : å› å­åç§°
        save_results : æ˜¯å¦ä¿å­˜éªŒè¯ç»“æœ
        
        Returns:
        --------
        éªŒè¯ç»“æœå­—å…¸
        """
        logger.info(f"ğŸ” å¿«é€ŸéªŒè¯å®éªŒæ€§å› å­: {factor_name}")
        
        validation = {
            'factor_name': factor_name,
            'validation_time': datetime.now(),
            'basic_stats': {},
            'data_quality': {},
            'recommendations': []
        }
        
        try:
            # 1. åŸºç¡€ç»Ÿè®¡
            validation['basic_stats'] = {
                'count': len(factor_data),
                'valid_count': factor_data.count(),
                'missing_rate': (len(factor_data) - factor_data.count()) / len(factor_data),
                'mean': factor_data.mean(),
                'std': factor_data.std(),
                'min': factor_data.min(),
                'max': factor_data.max(),
                'q25': factor_data.quantile(0.25),
                'median': factor_data.median(),
                'q75': factor_data.quantile(0.75),
                'skewness': factor_data.skew(),
                'kurtosis': factor_data.kurtosis()
            }
            
            # 2. æ•°æ®è´¨é‡æ£€æŸ¥
            stats = validation['basic_stats']
            
            # æ£€æŸ¥ç¼ºå¤±ç‡
            if stats['missing_rate'] > 0.5:
                validation['data_quality']['missing_rate'] = 'HIGH'
                validation['recommendations'].append('âš ï¸  ç¼ºå¤±ç‡è¿‡é«˜ï¼Œæ£€æŸ¥æ•°æ®æ¥æº')
            elif stats['missing_rate'] > 0.2:
                validation['data_quality']['missing_rate'] = 'MEDIUM'
                validation['recommendations'].append('âš¡ ç¼ºå¤±ç‡ä¸­ç­‰ï¼Œè€ƒè™‘æ•°æ®æ’è¡¥')
            else:
                validation['data_quality']['missing_rate'] = 'LOW'
            
            # æ£€æŸ¥æå€¼
            if abs(stats['skewness']) > 5:
                validation['data_quality']['skewness'] = 'HIGH'
                validation['recommendations'].append('ğŸ“Š ååº¦è¿‡å¤§ï¼Œè€ƒè™‘å»æå€¼å¤„ç†')
            
            if abs(stats['kurtosis']) > 10:
                validation['data_quality']['kurtosis'] = 'HIGH'
                validation['recommendations'].append('ğŸ“ˆ å³°åº¦è¿‡å¤§ï¼Œå­˜åœ¨æç«¯å€¼')
            
            # æ£€æŸ¥æ•°å€¼èŒƒå›´
            if np.isinf(stats['max']) or np.isinf(stats['min']):
                validation['data_quality']['infinite_values'] = True
                validation['recommendations'].append('ğŸš« å­˜åœ¨æ— ç©·å¤§å€¼ï¼Œæ£€æŸ¥è®¡ç®—é€»è¾‘')
            
            # 3. åˆ†å¸ƒç‰¹å¾
            valid_data = factor_data.dropna()
            if len(valid_data) > 100:
                # ç®€å•çš„æ­£æ€æ€§æ£€æŸ¥
                normal_test_stat = abs(stats['skewness']) + abs(stats['kurtosis'] - 3)
                if normal_test_stat < 2:
                    validation['data_quality']['distribution'] = 'NORMAL_LIKE'
                    validation['recommendations'].append('âœ… åˆ†å¸ƒæ¥è¿‘æ­£æ€')
                else:
                    validation['data_quality']['distribution'] = 'NON_NORMAL'
                    validation['recommendations'].append('ğŸ“Š éæ­£æ€åˆ†å¸ƒï¼Œå¯èƒ½éœ€è¦å˜æ¢')
            
            # 4. æ—¶é—´ç¨³å®šæ€§ï¼ˆå¦‚æœæœ‰æ—¶é—´ç´¢å¼•ï¼‰
            if isinstance(factor_data.index, pd.MultiIndex):
                dates = factor_data.index.get_level_values(0).unique()
                if len(dates) > 4:
                    # æŒ‰æ—¶é—´è®¡ç®—ç»Ÿè®¡é‡çš„å˜åŒ–
                    monthly_stats = []
                    for date in dates[-12:]:  # æœ€è¿‘12æœŸ
                        date_data = factor_data[factor_data.index.get_level_values(0) == date]
                        monthly_stats.append(date_data.mean())
                    
                    if len(monthly_stats) > 1:
                        stability_cv = np.std(monthly_stats) / np.mean(monthly_stats) if np.mean(monthly_stats) != 0 else np.inf
                        
                        if stability_cv < 0.1:
                            validation['data_quality']['time_stability'] = 'STABLE'
                            validation['recommendations'].append('â­ æ—¶é—´ç¨³å®šæ€§è‰¯å¥½')
                        elif stability_cv < 0.3:
                            validation['data_quality']['time_stability'] = 'MODERATE'
                            validation['recommendations'].append('âš¡ æ—¶é—´ç¨³å®šæ€§ä¸­ç­‰')
                        else:
                            validation['data_quality']['time_stability'] = 'UNSTABLE'
                            validation['recommendations'].append('âš ï¸  æ—¶é—´ä¸ç¨³å®šï¼Œæ£€æŸ¥è®¡ç®—é€»è¾‘')
            
            # 5. ç»¼åˆè¯„åˆ†
            score = 100
            if validation['data_quality'].get('missing_rate') == 'HIGH':
                score -= 30
            elif validation['data_quality'].get('missing_rate') == 'MEDIUM':
                score -= 15
            
            if validation['data_quality'].get('infinite_values'):
                score -= 25
            
            if validation['data_quality'].get('skewness') == 'HIGH':
                score -= 10
            
            if validation['data_quality'].get('time_stability') == 'UNSTABLE':
                score -= 20
            elif validation['data_quality'].get('time_stability') == 'STABLE':
                score += 5
            
            validation['overall_score'] = max(0, score)
            
            # 6. æœ€ç»ˆå»ºè®®
            if score >= 80:
                validation['recommendation'] = 'ğŸ‰ è´¨é‡è‰¯å¥½ï¼Œå»ºè®®æ·±åº¦æµ‹è¯•'
            elif score >= 60:
                validation['recommendation'] = 'âš¡ è´¨é‡ä¸­ç­‰ï¼Œéœ€è¦ä¼˜åŒ–åæµ‹è¯•'
            elif score >= 40:
                validation['recommendation'] = 'âš ï¸  å­˜åœ¨é—®é¢˜ï¼Œéœ€è¦é‡å¤§ä¿®æ”¹'
            else:
                validation['recommendation'] = 'âŒ è´¨é‡è¾ƒå·®ï¼Œå»ºè®®é‡æ–°è®¾è®¡'
            
            # ä¿å­˜ç»“æœ
            if save_results:
                self.validation_results[factor_name] = validation
            
            # è¾“å‡ºæŠ¥å‘Š
            self._print_validation_report(validation)
            
        except Exception as e:
            logger.error(f"å› å­éªŒè¯å¤±è´¥: {e}")
            validation['error'] = str(e)
        
        return validation
    
    def _print_validation_report(self, validation: Dict[str, Any]):
        """æ‰“å°éªŒè¯æŠ¥å‘Š"""
        print("=" * 60)
        print(f"ğŸ§ª å®éªŒæ€§å› å­éªŒè¯æŠ¥å‘Š: {validation['factor_name']}")
        print("=" * 60)
        
        stats = validation['basic_stats']
        print(f"ğŸ“Š åŸºç¡€ç»Ÿè®¡:")
        print(f"   æ•°æ®ç‚¹æ•°: {stats['count']:,}")
        print(f"   æœ‰æ•ˆæ•°æ®: {stats['valid_count']:,} ({(1-validation['basic_stats']['missing_rate'])*100:.1f}%)")
        print(f"   å‡å€¼: {stats['mean']:.6f}")
        print(f"   æ ‡å‡†å·®: {stats['std']:.6f}")
        print(f"   åˆ†ä½æ•°: [{stats['q25']:.4f}, {stats['median']:.4f}, {stats['q75']:.4f}]")
        
        print(f"\nğŸ” æ•°æ®è´¨é‡:")
        for key, value in validation['data_quality'].items():
            print(f"   {key}: {value}")
        
        print(f"\nâ­ ç»¼åˆè¯„åˆ†: {validation['overall_score']}/100")
        print(f"ğŸ“‹ æ€»ä½“å»ºè®®: {validation['recommendation']}")
        
        if validation['recommendations']:
            print(f"\nğŸ’¡ å…·ä½“å»ºè®®:")
            for rec in validation['recommendations']:
                print(f"   {rec}")
        
        print("=" * 60)
    
    def run_experimental_batch(self, 
                              financial_data: pd.DataFrame,
                              factor_list: List[str] = None) -> Dict[str, pd.Series]:
        """
        æ‰¹é‡è¿è¡Œå®éªŒæ€§å› å­
        
        Parameters:
        -----------
        financial_data : è´¢åŠ¡æ•°æ®
        factor_list : è¦æµ‹è¯•çš„å› å­åˆ—è¡¨ï¼ŒNoneè¡¨ç¤ºæ‰€æœ‰å®éªŒæ€§å› å­
        
        Returns:
        --------
        å› å­ç»“æœå­—å…¸
        """
        if factor_list is None:
            # è‡ªåŠ¨å‘ç°æ‰€æœ‰EXPERIMENTAL_å¼€å¤´çš„æ–¹æ³•
            factor_list = [method for method in dir(self) 
                          if method.startswith('calculate_EXPERIMENTAL_')]
        
        results = {}
        logger.info(f"ğŸš€ å¼€å§‹æ‰¹é‡æµ‹è¯• {len(factor_list)} ä¸ªå®éªŒæ€§å› å­")
        
        for method_name in factor_list:
            factor_name = method_name.replace('calculate_EXPERIMENTAL_', '').replace('_ttm', '')
            
            try:
                method = getattr(self, method_name)
                logger.info(f"   è®¡ç®—å› å­: {factor_name}")
                
                factor_data = method(financial_data)
                results[factor_name] = factor_data
                
                # å¿«é€ŸéªŒè¯
                self.quick_validate_factor(factor_data, factor_name, save_results=True)
                
            except Exception as e:
                logger.error(f"   âŒ {factor_name} è®¡ç®—å¤±è´¥: {e}")
                results[factor_name] = None
        
        logger.info(f"âœ… æ‰¹é‡æµ‹è¯•å®Œæˆï¼ŒæˆåŠŸè®¡ç®— {sum(1 for v in results.values() if v is not None)} ä¸ªå› å­")
        return results
    
    def export_to_production(self, 
                            experimental_method_name: str,
                            production_name: str,
                            category: str = 'profitability') -> str:
        """
        ç”Ÿæˆå°†å®éªŒæ€§å› å­è¿ç§»åˆ°ç”Ÿäº§ç¯å¢ƒçš„ä»£ç æ¨¡æ¿
        
        Parameters:
        -----------
        experimental_method_name : å®éªŒæ€§æ–¹æ³•å
        production_name : ç”Ÿäº§ç¯å¢ƒå› å­å
        category : å› å­åˆ†ç±»
        
        Returns:
        --------
        è¿ç§»ä»£ç æ¨¡æ¿
        """
        template = f'''
# å°† {experimental_method_name} è¿ç§»åˆ° pure_financial_factors.py

# 1. åœ¨ PureFinancialFactorCalculator ç±»ä¸­æ·»åŠ æ–¹æ³•ï¼š
def calculate_{production_name}_ttm(self, financial_data: pd.DataFrame, **kwargs) -> pd.Series:
    """
    {production_name}å› å­ï¼ˆç»éªŒè¯æœ‰æ•ˆï¼‰
    
    è¿ç§»è‡ªå®éªŒæ€§å› å­: {experimental_method_name}
    """
    # ä»å®éªŒæ€§æ¨¡å—å¤åˆ¶å®ç°ä»£ç 
    pass

# 2. åœ¨ _register_all_factors æ–¹æ³•ä¸­æ³¨å†Œï¼š
methods.update({{
    '{production_name}_ttm': self.calculate_{production_name}_ttm,
}})

# 3. åœ¨ factor_categories ä¸­æ·»åŠ åˆ°åˆ†ç±»ï¼š
self.factor_categories['{category}'].append('{production_name}_ttm')

# 4. åœ¨ get_factor_info ä¸­æ·»åŠ è¯´æ˜ï¼š
'{production_name}_ttm': '{production_name} - [æ·»åŠ å› å­è¯´æ˜]',

# 5. åˆ é™¤å®éªŒæ€§ç‰ˆæœ¬ï¼š
# ä» experimental_factors.py ä¸­åˆ é™¤ {experimental_method_name} æ–¹æ³•
        '''
        
        print(template)
        return template
    
    # =====================================================
    # å­—æ®µéªŒè¯å’Œè¯´æ˜å·¥å…·
    # =====================================================
    
    def validate_and_explain_fields(self, field_names: List[str]) -> Dict:
        """
        éªŒè¯å­—æ®µå¹¶æä¾›ä¸­æ–‡è¯´æ˜
        
        Parameters:
        -----------
        field_names : list
            å­—æ®µååˆ—è¡¨
            
        Returns:
        --------
        dict
            éªŒè¯ç»“æœå’Œå­—æ®µè¯´æ˜
        """
        results = {
            'validation': {},
            'explanations': {},
            'missing_fields': [],
            'available_fields': []
        }
        
        for field_name in field_names:
            field_info = self.field_mapper.get_field_info(field_name)
            
            if field_info:
                results['validation'][field_name] = True
                results['explanations'][field_name] = {
                    'chinese_name': field_info['chinese_name'],
                    'table': field_info['table_chinese'] or field_info['table'],
                    'table_en': field_info['table']
                }
                results['available_fields'].append(field_name)
            else:
                results['validation'][field_name] = False
                results['missing_fields'].append(field_name)
        
        return results
    
    def search_similar_fields(self, keyword: str, max_results: int = 10) -> List[Dict]:
        """
        æœç´¢ç›¸ä¼¼å­—æ®µ
        
        Parameters:
        -----------
        keyword : str
            æœç´¢å…³é”®è¯
        max_results : int
            æœ€å¤§ç»“æœæ•°
            
        Returns:
        --------
        list
            ç›¸ä¼¼å­—æ®µåˆ—è¡¨
        """
        return self.field_mapper.search_fields(keyword)[:max_results]
    
    def print_field_usage_report(self, field_names: List[str]):
        """
        æ‰“å°å­—æ®µä½¿ç”¨æŠ¥å‘Š
        
        Parameters:
        -----------
        field_names : list
            ä½¿ç”¨çš„å­—æ®µåˆ—è¡¨
        """
        results = self.validate_and_explain_fields(field_names)
        
        print("=" * 60)
        print("å­—æ®µä½¿ç”¨æŠ¥å‘Š")
        print("=" * 60)
        
        if results['available_fields']:
            print("âœ… å¯ç”¨å­—æ®µ:")
            for field_name in results['available_fields']:
                info = results['explanations'][field_name]
                print(f"   {field_name} -> {info['chinese_name']} ({info['table']})")
        
        if results['missing_fields']:
            print("âŒ æœªæ‰¾åˆ°å­—æ®µ:")
            for field_name in results['missing_fields']:
                print(f"   {field_name}")
                # å°è¯•æœç´¢ç›¸ä¼¼å­—æ®µ
                similar = self.search_similar_fields(field_name, 3)
                if similar:
                    print(f"     å»ºè®®ä½¿ç”¨: {', '.join([s['field_name'] for s in similar])}")
        
        print("=" * 60)
    
    # =====================================================
    # ä½ çš„å®éªŒæ€§å› å­ä»è¿™é‡Œå¼€å§‹æ·»åŠ 
    # =====================================================
    
    # TODO: åœ¨è¿™é‡Œæ·»åŠ ä½ çš„å®éªŒæ€§å› å­
    # å¤åˆ¶ä¸Šé¢çš„æ¨¡æ¿ï¼Œä¿®æ”¹æ–¹æ³•åå’Œè®¡ç®—é€»è¾‘
    
    pass


# =====================================================
# ä¾¿æ·å‡½æ•°
# =====================================================

def create_experimental_factor_template(factor_name: str, 
                                       formula_description: str,
                                       economic_meaning: str,
                                       hypothesis: str) -> str:
    """
    ç”Ÿæˆå®éªŒæ€§å› å­ä»£ç æ¨¡æ¿
    
    Parameters:
    -----------
    factor_name : å› å­åç§°
    formula_description : è®¡ç®—å…¬å¼æè¿°
    economic_meaning : ç»æµå«ä¹‰
    hypothesis : éªŒè¯å‡è®¾
    
    Returns:
    --------
    ä»£ç æ¨¡æ¿å­—ç¬¦ä¸²
    """
    template = f'''
def calculate_EXPERIMENTAL_{factor_name}_ttm(self, financial_data: pd.DataFrame, **kwargs) -> pd.Series:
    """
    å®éªŒæ€§å› å­ï¼š{factor_name}
    
    è®¡ç®—å…¬å¼ï¼š{formula_description}
    ç»æµå«ä¹‰ï¼š{economic_meaning}
    å‡è®¾éªŒè¯ï¼š{hypothesis}
    
    Parameters:
    -----------
    financial_data : pd.DataFrame
        è´¢åŠ¡æ•°æ®
    **kwargs : dict
        å…¶ä»–å‚æ•°
        
    Returns:
    --------
    pd.Series
        å› å­å€¼
    """
    try:
        # æ­¥éª¤1ï¼šéªŒè¯æ•°æ®éœ€æ±‚
        required_cols = ['earnings', 'revenue']  # ä¿®æ”¹ä¸ºä½ éœ€è¦çš„åˆ—
        if not self.validate_data_requirements(financial_data, required_cols):
            raise ValueError(f"Required data not available for {factor_name} calculation")
        
        # æ­¥éª¤2ï¼šæå–æ•°æ®
        extracted_data = self.extract_required_data(financial_data, required_cols)
        
        # æ­¥éª¤3ï¼šå®ç°è®¡ç®—é€»è¾‘
        # TODO: åœ¨è¿™é‡Œå®ç°ä½ çš„å› å­è®¡ç®—
        result = pd.Series(index=financial_data.index, dtype=float)
        
        # æ­¥éª¤4ï¼šè¿”å›ç»“æœ
        logger.info("âœ¨ å®éªŒæ€§å› å­{factor_name}è®¡ç®—å®Œæˆ")
        return result
        
    except Exception as e:
        logger.error(f"{factor_name}è®¡ç®—å¤±è´¥: {{e}}")
        return pd.Series(index=financial_data.index, dtype=float)
'''
    
    return template


def quick_factor_test(factor_method, financial_data: pd.DataFrame, factor_name: str = "TestFactor"):
    """
    å¿«é€Ÿæµ‹è¯•å› å­å‡½æ•°
    
    Parameters:
    -----------
    factor_method : å› å­è®¡ç®—å‡½æ•°
    financial_data : è´¢åŠ¡æ•°æ®
    factor_name : å› å­åç§°
    """
    calculator = ExperimentalFactorCalculator()
    
    print(f"ğŸ§ª å¿«é€Ÿæµ‹è¯•å› å­: {factor_name}")
    
    try:
        result = factor_method(financial_data)
        validation = calculator.quick_validate_factor(result, factor_name, save_results=False)
        return result, validation
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        return None, None


def main():
    project_root = Path(__file__).parent
    data_path = project_root / "data" / "auxiliary" / "FinancialData_unified.pkl"
    data_path1 =  r"E:\Documents\PythonProject\StockProject\StockData\LogReturn_daily_o2o.pkl"       
    if not data_path.exists():
        logger.error(f"è´¢åŠ¡æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {data_path}")
        return None
        
    financial_data = pd.read_pickle(data_path)
    log_return_data = pd.read_pickle(data_path1)
    experimentfactor = ExperimentalFactorCalculator()


# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    print("ğŸ§ª å®éªŒæ€§å› å­æ¨¡å—")
    print("ä½¿ç”¨æ–¹æ³•:")
    print("1. å¤åˆ¶æ¨¡æ¿åˆ›å»ºæ–°å› å­")
    print("2. ä½¿ç”¨ quick_validate_factor éªŒè¯")
    print("3. ä½¿ç”¨ export_to_production è¿ç§»åˆ°æ­£å¼ç¯å¢ƒ")